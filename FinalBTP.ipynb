{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalWorkingBTP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMstvjidahhCfCPbb9/rnZk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreya0505/MusicalSourceSeparation/blob/master/FinalBTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmeliD_s8gUt",
        "colab_type": "text"
      },
      "source": [
        "# **Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjF4GnMt8kzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "aeaff2d7-0695-4a2a-a816-6d108e770d62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8kx-eOaExE8",
        "colab_type": "text"
      },
      "source": [
        "# **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj_bmCX7EzoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args:\n",
        "  num_workers = 1\n",
        "  features = 32\n",
        "  log_dir = '/content/drive/My Drive/BTP/Dataset/logs'\n",
        "  dataset_dir = '/content/drive/My Drive/BTP/Dataset'\n",
        "  hdf_dir = '/content/drive/My Drive/BTP/Dataset/hdf'\n",
        "  checkpoint_dir = 'checkpoints/waveunet'\n",
        "  load_model = None\n",
        "  lr = 1e-3\n",
        "  min_lr = 5e-5\n",
        "  cycles = 2\n",
        "  batch_size = 4\n",
        "  levels = 6\n",
        "  depth = 1\n",
        "  sr = 44100\n",
        "  channels = 2\n",
        "  kernel_size = 5\n",
        "  output_size = 2.0\n",
        "  strides = 4\n",
        "  patience = 20\n",
        "  example_freq = 200\n",
        "  loss = 'L1'\n",
        "  conv_type = 'gn'\n",
        "  res = 'fixed'\n",
        "  separate = 1\n",
        "  feature_growth = 'double'\n",
        "  instruments = [\"bass\", \"drums\", \"others\", \"vocals\"]\n",
        "  \n",
        "args=Args()\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox-NgWKAD9Y_",
        "colab_type": "text"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmOXBb40FM_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "1f5d5a6c-fd1e-4b3f-e727-cc001c3505cf"
      },
      "source": [
        "pip install soundfile"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDipAMXJFT8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "c8e360b6-668d-4955-9e42-d6b852430b65"
      },
      "source": [
        "pip install stempeg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stempeg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/ab/6e7362cbff21c25e99cfc3ef116057a7f9ebe6f429a44038eef82de3479d/stempeg-0.1.8-py3-none-any.whl (509kB)\n",
            "\r\u001b[K     |▋                               | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 7.0MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 6.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 286kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 296kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 307kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 317kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 327kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 337kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 348kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 358kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 368kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 378kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 389kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 399kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 409kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 419kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 430kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 440kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 450kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 460kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 471kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 481kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 491kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 501kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 512kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from stempeg) (1.18.5)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from stempeg) (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->stempeg) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->stempeg) (2.20)\n",
            "Installing collected packages: stempeg\n",
            "Successfully installed stempeg-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckVzN4mEFYdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "03b91aa9-721c-4f23-d6cc-9b8269cfae92"
      },
      "source": [
        "pip install ffprobe"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffprobe\n",
            "  Downloading https://files.pythonhosted.org/packages/95/9c/adf90d21108d41f611aa921defd2f2e56d3f92724e4b5aa41fae7a9972aa/ffprobe-0.5.zip\n",
            "Building wheels for collected packages: ffprobe\n",
            "  Building wheel for ffprobe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffprobe: filename=ffprobe-0.5-cp36-none-any.whl size=3432 sha256=c7b954c8f43e330579cd6297f4cdbc849cacc1624337939549717157e9f02cbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/fe/3b/8af08ae1cdfc27e226779e2cb1a7c8a2ba4954c05c562cdc77\n",
            "Successfully built ffprobe\n",
            "Installing collected packages: ffprobe\n",
            "Successfully installed ffprobe-0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4SBbs0_Fbqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "4e5cc2dc-61a7-41dd-e8ae-231d47c62c15"
      },
      "source": [
        "pip install ffmpeg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/cc/3b7408b8ecf7c1d20ad480c3eaed7619857bf1054b690226e906fdf14258/ffmpeg-1.4.tar.gz\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-cp36-none-any.whl size=6084 sha256=ea9ee412d7132399faa4bb482566367f3cbc0431a0a75db59c0fbc9a478ef5b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/68/c3/a05a35f647ba871e5572b9bbfc0b95fd1c6637a2219f959e7a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg3aGLmJtpD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "b1587aed-0252-4045-b271-c764d7ae0181"
      },
      "source": [
        "pip install museval"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting museval\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/5d/1419271e96537ea973d039818c7f164a150e68867fd216fd591d0a5e43b7/museval-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from museval) (2.6.0)\n",
            "Collecting musdb>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/bd/98ba16482f610bcfa7fcc212175dc0bbf11976e0bc69319b4204b6dc3aec/musdb-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (from museval) (0.10.3.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from museval) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from museval) (1.4.1)\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.6/dist-packages (from museval) (1.0.5)\n",
            "Requirement already satisfied: stempeg>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from musdb>=0.3.0->museval) (0.1.8)\n",
            "Collecting pyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from musdb>=0.3.0->museval) (4.41.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile->museval) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.0->museval) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.0->museval) (2018.9)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml->musdb>=0.3.0->museval) (3.13)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile->museval) (2.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.25.0->museval) (1.15.0)\n",
            "Installing collected packages: pyaml, musdb, simplejson, museval\n",
            "Successfully installed musdb-0.3.1 museval-0.3.0 pyaml-20.4.0 simplejson-3.17.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tUJbR7xEG5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "import stempeg\n",
        "import os\n",
        "import glob2 as glob\n",
        "import numpy as np\n",
        "import csv \n",
        "from tqdm import tqdm\n",
        "from sortedcontainers import SortedList\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "from functools import partial\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import Adam\n",
        "import pickle\n",
        "import museval\n",
        "import librosa\n",
        "import soundfile"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upOxvG4K8lPj",
        "colab_type": "text"
      },
      "source": [
        "# **Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFj_3Dqnv7P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def worker_init_fn(worker_id): # This is apparently needed to ensure workers have different random seeds and draw different examples!\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZrbDJZS-T1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_output(model, inputs):\n",
        "    '''\n",
        "    Computes outputs of model with given inputs. Does NOT allow propagating gradients! See compute_loss for training.\n",
        "    Procedure depends on whether we have one model for each source or not\n",
        "    :param model: Model to train with\n",
        "    :param compute_grad: Whether to compute gradients\n",
        "    :return: Model outputs, Average loss over batch\n",
        "    '''\n",
        "    all_outputs = {}\n",
        "\n",
        "    if model.separate:\n",
        "        for inst in model.instruments:\n",
        "            output = model(inputs, inst)\n",
        "            all_outputs[inst] = output[inst].detach().clone()\n",
        "    else:\n",
        "        all_outputs = model(inputs)\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "def compute_loss(model, inputs, targets, criterion, compute_grad=False):\n",
        "    '''\n",
        "    Computes gradients of model with given inputs and targets and loss function.\n",
        "    Optionally backpropagates to compute gradients for weights.\n",
        "    Procedure depends on whether we have one model for each source or not\n",
        "    :param model: Model to train with\n",
        "    :param inputs: Input mixture\n",
        "    :param targets: Target sources\n",
        "    :param criterion: Loss function to use (L1, L2, ..)\n",
        "    :param compute_grad: Whether to compute gradients\n",
        "    :return: Model outputs, Average loss over batch\n",
        "    '''\n",
        "    all_outputs = {}\n",
        "\n",
        "    if model.separate:\n",
        "        avg_loss = 0.0\n",
        "        num_sources = 0\n",
        "        for inst in model.instruments:\n",
        "            output = model(inputs, inst)\n",
        "            loss = criterion(output[inst], targets[inst])\n",
        "\n",
        "            if compute_grad:\n",
        "                loss.backward()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            num_sources += 1\n",
        "\n",
        "            all_outputs[inst] = output[inst].detach().clone()\n",
        "\n",
        "        avg_loss /= float(num_sources)\n",
        "    else:\n",
        "        loss = 0\n",
        "        all_outputs = model(inputs)\n",
        "        for inst in all_outputs.keys():\n",
        "            loss += criterion(all_outputs[inst], targets[inst])\n",
        "\n",
        "        if compute_grad:\n",
        "            loss.backward()\n",
        "\n",
        "        avg_loss = loss.item() / float(len(all_outputs))\n",
        "\n",
        "    return all_outputs, avg_loss\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmmWIrco98Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resample(audio, orig_sr, new_sr, mode=\"numpy\"):\n",
        "    if orig_sr == new_sr:\n",
        "        return audio\n",
        "\n",
        "    if isinstance(audio, torch.Tensor):\n",
        "        audio = audio.detach().cpu().numpy()\n",
        "\n",
        "    out = librosa.resample(audio, orig_sr, new_sr, res_type='kaiser_fast')\n",
        "\n",
        "    if mode == \"pytorch\":\n",
        "        out = torch.tensor(out)\n",
        "    return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhTrh5BK8rio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model, optimizer, path):\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model = model.module  # load state dict of wrapped module\n",
        "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "    try:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    except:\n",
        "        # work-around for loading checkpoints where DataParallel was saved instead of inner module\n",
        "        from collections import OrderedDict\n",
        "        model_state_dict_fixed = OrderedDict()\n",
        "        prefix = 'module.'\n",
        "        for k, v in checkpoint['model_state_dict'].items():\n",
        "            if k.startswith(prefix):\n",
        "                k = k[len(prefix):]\n",
        "            model_state_dict_fixed[k] = v\n",
        "        model.load_state_dict(model_state_dict_fixed)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if 'state' in checkpoint:\n",
        "        state = checkpoint['state']\n",
        "    else:\n",
        "        # older checkpoitns only store step, rest of state won't be there\n",
        "        state = {'step': checkpoint['step']}\n",
        "    return state"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q3T83bI6mtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load(path, sr=22050, mono=True, mode=\"numpy\", offset=0.0, duration=None):\n",
        "    y, curr_sr = librosa.load(path, sr=sr, mono=mono, res_type='kaiser_fast', offset=offset, duration=duration)\n",
        "\n",
        "    if len(y.shape) == 1:\n",
        "        # Expand channel dimension\n",
        "        y = y[np.newaxis, :]\n",
        "\n",
        "    if mode == \"pytorch\":\n",
        "        y = torch.tensor(y)\n",
        "\n",
        "    return y, curr_sr\n",
        "\n",
        "def write_wav(path, audio, sr):\n",
        "    soundfile.write(path, audio.T, sr, \"PCM_16\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STfIAvxJfQaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_cyclic_lr(optimizer, it, epoch_it, cycles, min_lr, max_lr):\n",
        "    cycle_length = epoch_it // cycles\n",
        "    curr_cycle = min(it // cycle_length, cycles-1)\n",
        "    curr_it = it - cycle_length * curr_cycle\n",
        "\n",
        "    new_lr = min_lr + 0.5*(max_lr - min_lr)*(1 + np.cos((float(curr_it) / float(cycle_length)) * np.pi))\n",
        "    set_lr(optimizer, new_lr)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WISPmDsgfqPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lr(optim):\n",
        "    return optim.param_groups[0][\"lr\"]\n",
        "\n",
        "def set_lr(optim, lr):\n",
        "    for g in optim.param_groups:\n",
        "        g['lr'] = lr\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0GBFGS5FqZV",
        "colab_type": "text"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4D6YHVJFshp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getMUSDBHQ(directory):\n",
        "    subsets = list()\n",
        "\n",
        "    for subset in [\"train\", \"test\"]:\n",
        "        subset_path = os.path.join(directory,subset)\n",
        "        samples = list()\n",
        "\n",
        "        for track in os.listdir(subset_path):\n",
        "            audio_path = os.path.join(subset_path,track)\n",
        "            \n",
        "            track_name= track[:-9]\n",
        "            target_path = os.path.join(directory,\"separated\",subset,track_name)\n",
        "\n",
        "            example = dict()\n",
        "            for stem in [\"mix\", \"drums\", \"bass\", \"others\",\"vocals\"]:\n",
        "              stem_path = os.path.join(target_path,stem + \".wav\")\n",
        "              example[stem] = stem_path\n",
        "             \n",
        "            acc_path = stem_path = os.path.join(target_path,  \"accompaniment.wav\")\n",
        "            example[\"accompaniment\"] = acc_path\n",
        "            \n",
        "            samples.append(example)\n",
        "\n",
        "        subsets.append(samples)\n",
        "\n",
        "    return subsets\n",
        "\n",
        "\n",
        "def get_musdb_folds(root_path):\n",
        "    dataset = getMUSDBHQ(root_path)\n",
        "    train_val_list = dataset[0]\n",
        "    test_list = dataset[1]\n",
        "\n",
        "    np.random.seed(1337)\n",
        "    train_list = np.random.choice(train_val_list, 75, replace=False)\n",
        "    val_list = [elem for elem in train_val_list if elem not in train_list]\n",
        "    output = dict()\n",
        "    output = {\"train\" : train_list, \"val\" : val_list, \"test\" : test_list} \n",
        "    \n",
        "    output_path= os.path.join(root_path, \"outputs\" ,\"get_musdb_folds.txt\")\n",
        "    \n",
        "    if os.path.exists(output_path):\n",
        "          os.remove(output_path)\n",
        "    log = open(output_path,\"a\")\n",
        "    for list_type in output:\n",
        "      log.write(\"\\n\" + \"--\"*35 + \"\\n\\n\")\n",
        "      log.write(list_type)\n",
        "      log.write(\"\\t\\t\\t\"+ \"No of songs:\"+ str(len(output[list_type]))) \n",
        "      log.write(\"\\n\" + \"--\"*35 + \"\\n\\n\")\n",
        "      \n",
        "      for dictionary in output[list_type]:\n",
        "        path = (dictionary[\"mix\"])\n",
        "        path = path[:-8]\n",
        "        log.write(path+\"\\n\")\n",
        "        \n",
        "    log.close()\n",
        "\n",
        "    return output\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Lj5VgtW-ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_sep(mix, targets, shapes):\n",
        "    '''\n",
        "    Crops target audio to the output shape required by the model given in \"shapes\"\n",
        "    '''\n",
        "    for key in targets.keys():\n",
        "        if key != \"mix\":\n",
        "            targets[key] = targets[key][:, shapes[\"output_start_frame\"]:shapes[\"output_end_frame\"]]\n",
        "    return mix, targets\n",
        "\n",
        "def random_amplify(mix, targets, shapes, min, max):\n",
        "    '''\n",
        "    Data augmentation by randomly amplifying sources before adding them to form a new mixture\n",
        "    :param mix: Original mixture\n",
        "    :param targets: Source targets\n",
        "    :param shapes: Shape dict from model\n",
        "    :param min: Minimum possible amplification\n",
        "    :param max: Maximum possible amplification\n",
        "    :return: New data point as tuple (mix, targets)\n",
        "    '''\n",
        "    residual = mix  # start with original mix\n",
        "    for key in targets.keys():\n",
        "        if key != \"mix\":\n",
        "            residual -= targets[key]  # subtract all instruments (output is zero if all instruments add to mix)\n",
        "    mix = residual * np.random.uniform(min, max)  # also apply gain data augmentation to residual\n",
        "    for key in targets.keys():\n",
        "        if key != \"mix\":\n",
        "            targets[key] = targets[key] * np.random.uniform(min, max)\n",
        "            mix += targets[key]  # add instrument with gain data augmentation to mix\n",
        "    mix = np.clip(mix, -1.0, 1.0)\n",
        "    return crop_sep(mix, targets, shapes)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPQDJfJtU_5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeparationDataset(Dataset):\n",
        "    def __init__(self, dataset, partition, instruments, sr, channels, shapes, random_hops, hdf_dir, audio_transform=None, in_memory=False):\n",
        "        '''\n",
        "        :param data: HDF audio data object\n",
        "        :param input_size: Number of input samples for each example\n",
        "        :param context_front: Number of extra context samples to prepend to input\n",
        "        :param context_back: NUmber of extra context samples to append to input\n",
        "        :param hop_size: Skip hop_size - 1 sample positions in the audio for each example (subsampling the audio)\n",
        "        :param random_hops: If False, sample examples evenly from whole audio signal according to hop_size parameter. If True, randomly sample a position from the audio\n",
        "        '''\n",
        "\n",
        "        super(SeparationDataset, self).__init__()\n",
        "\n",
        "        self.hdf_dataset = None\n",
        "        os.makedirs(hdf_dir, exist_ok=True)\n",
        "        self.hdf_dir = os.path.join(hdf_dir, partition + \".hdf5\")\n",
        "\n",
        "        self.random_hops = random_hops\n",
        "        self.sr = sr\n",
        "        self.channels = channels\n",
        "        self.shapes = shapes\n",
        "        self.audio_transform = audio_transform\n",
        "        self.in_memory = in_memory\n",
        "        self.instruments = instruments\n",
        "\n",
        "        # PREPARE HDF FILE\n",
        "\n",
        "        # Check if HDF file exists already\n",
        "        if not os.path.exists(self.hdf_dir):\n",
        "            # Create folder if it did not exist before\n",
        "            if not os.path.exists(hdf_dir):\n",
        "                os.makedirs(hdf_dir)\n",
        "\n",
        "            # Create HDF file\n",
        "            with h5py.File(self.hdf_dir, \"w\") as f:\n",
        "                f.attrs[\"sr\"] = sr\n",
        "                f.attrs[\"channels\"] = channels\n",
        "                f.attrs[\"instruments\"] = instruments\n",
        "\n",
        "                print(\"Adding audio files to dataset (preprocessing)...\")\n",
        "                for idx, example in enumerate(tqdm(dataset[partition])):\n",
        "                    # Load mix\n",
        "                    mix_audio, _ = load(example[\"mix\"], sr=self.sr, mono=(self.channels == 1))\n",
        "\n",
        "                    source_audios = []\n",
        "                    for source in instruments:\n",
        "                        # In this case, read in audio and convert to target sampling rate\n",
        "                        source_audio, _ = load(example[source], sr=self.sr, mono=(self.channels == 1))\n",
        "                        source_audios.append(source_audio)\n",
        "                    source_audios = np.concatenate(source_audios, axis=0)\n",
        "                    assert(source_audios.shape[1] == mix_audio.shape[1])\n",
        "\n",
        "                    # Add to HDF5 file\n",
        "                    grp = f.create_group(str(idx))\n",
        "                    grp.create_dataset(\"inputs\", shape=mix_audio.shape, dtype=mix_audio.dtype, data=mix_audio)\n",
        "                    grp.create_dataset(\"targets\", shape=source_audios.shape, dtype=source_audios.dtype, data=source_audios)\n",
        "                    grp.attrs[\"length\"] = mix_audio.shape[1]\n",
        "                    grp.attrs[\"target_length\"] = source_audios.shape[1]\n",
        "\n",
        "        # In that case, check whether sr and channels are complying with the audio in the HDF file, otherwise raise error\n",
        "        with h5py.File(self.hdf_dir, \"r\") as f:\n",
        "            if f.attrs[\"sr\"] != sr or \\\n",
        "                    f.attrs[\"channels\"] != channels or \\\n",
        "                    list(f.attrs[\"instruments\"]) != instruments:\n",
        "                raise ValueError(\n",
        "                    \"Tried to load existing HDF file, but sampling rate and channel or instruments are not as expected. Did you load an out-dated HDF file?\")\n",
        "\n",
        "        # HDF FILE READY\n",
        "\n",
        "        # SET SAMPLING POSITIONS\n",
        "\n",
        "        # Go through HDF and collect lengths of all audio files\n",
        "        with h5py.File(self.hdf_dir, \"r\") as f:\n",
        "            lengths = [f[str(song_idx)].attrs[\"target_length\"] for song_idx in range(len(f))]\n",
        "\n",
        "            # Subtract input_size from lengths and divide by hop size to determine number of starting positions\n",
        "            lengths = [(l // self.shapes[\"output_frames\"]) + 1 for l in lengths]\n",
        "\n",
        "        self.start_pos = SortedList(np.cumsum(lengths))\n",
        "        self.length = self.start_pos[-1]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Open HDF5\n",
        "        if self.hdf_dataset is None:\n",
        "            driver = \"core\" if self.in_memory else None  # Load HDF5 fully into memory if desired\n",
        "            self.hdf_dataset = h5py.File(self.hdf_dir, 'r', driver=driver)\n",
        "\n",
        "        # Find out which slice of targets we want to read\n",
        "        audio_idx = self.start_pos.bisect_right(index)\n",
        "        if audio_idx > 0:\n",
        "            index = index - self.start_pos[audio_idx - 1]\n",
        "\n",
        "        # Check length of audio signal\n",
        "        audio_length = self.hdf_dataset[str(audio_idx)].attrs[\"length\"]\n",
        "        target_length = self.hdf_dataset[str(audio_idx)].attrs[\"target_length\"]\n",
        "\n",
        "        # Determine position where to start targets\n",
        "        if self.random_hops:\n",
        "            start_target_pos = np.random.randint(0, max(target_length - self.shapes[\"output_frames\"] + 1, 1))\n",
        "        else:\n",
        "            # Map item index to sample position within song\n",
        "            start_target_pos = index * self.shapes[\"output_frames\"]\n",
        "\n",
        "        # READ INPUTS\n",
        "        # Check front padding\n",
        "        start_pos = start_target_pos - self.shapes[\"output_start_frame\"]\n",
        "        if start_pos < 0:\n",
        "            # Pad manually since audio signal was too short\n",
        "            pad_front = abs(start_pos)\n",
        "            start_pos = 0\n",
        "        else:\n",
        "            pad_front = 0\n",
        "\n",
        "        # Check back padding\n",
        "        end_pos = start_target_pos - self.shapes[\"output_start_frame\"] + self.shapes[\"input_frames\"]\n",
        "        if end_pos > audio_length:\n",
        "            # Pad manually since audio signal was too short\n",
        "            pad_back = end_pos - audio_length\n",
        "            end_pos = audio_length\n",
        "        else:\n",
        "            pad_back = 0\n",
        "\n",
        "        # Read and return\n",
        "        audio = self.hdf_dataset[str(audio_idx)][\"inputs\"][:, start_pos:end_pos].astype(np.float32)\n",
        "        if pad_front > 0 or pad_back > 0:\n",
        "            audio = np.pad(audio, [(0, 0), (pad_front, pad_back)], mode=\"constant\", constant_values=0.0)\n",
        "\n",
        "        targets = self.hdf_dataset[str(audio_idx)][\"targets\"][:, start_pos:end_pos].astype(np.float32)\n",
        "        if pad_front > 0 or pad_back > 0:\n",
        "            targets = np.pad(targets, [(0, 0), (pad_front, pad_back)], mode=\"constant\", constant_values=0.0)\n",
        "\n",
        "        targets = {inst : targets[idx*self.channels:(idx+1)*self.channels] for idx, inst in enumerate(self.instruments)}\n",
        "\n",
        "        if hasattr(self, \"audio_transform\") and self.audio_transform is not None:\n",
        "            audio, targets = self.audio_transform(audio, targets)\n",
        "\n",
        "        return audio, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS2XceU88rzb",
        "colab_type": "text"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8-Go6iU8vYn",
        "colab_type": "text"
      },
      "source": [
        "### **Model Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgqAv4K7817Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop(x, target):\n",
        "    '''\n",
        "    Center-crop 3-dim. input tensor along last axis so it fits the target tensor shape\n",
        "    :param x: Input tensor\n",
        "    :param target: Shape of this tensor will be used as target shape\n",
        "    :return: Cropped input tensor\n",
        "    '''\n",
        "    if x is None:\n",
        "        return None\n",
        "    if target is None:\n",
        "        return x\n",
        "\n",
        "    target_shape = target.shape\n",
        "    diff = x.shape[-1] - target_shape[-1]\n",
        "    assert (diff % 2 == 0)\n",
        "    crop = diff // 2\n",
        "\n",
        "    if crop == 0:\n",
        "        return x\n",
        "    if crop < 0:\n",
        "        raise ArithmeticError\n",
        "\n",
        "    return x[:, :, crop:-crop].contiguous()\n",
        "\n",
        "def sinc(x):\n",
        "    x[np.abs(x) < 1e-20] = 1e-20\n",
        "    return np.sin(np.pi * x) / (np.pi * x)\n",
        "\n",
        "def build_sinc_filter(kernel_size, cutoff):\n",
        "    # FOLLOWING https://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch16.pdf\n",
        "    # Sinc lowpass filter\n",
        "    # Build sinc kernel\n",
        "    assert(kernel_size % 2 == 1)\n",
        "    M = kernel_size - 1\n",
        "    filter = np.zeros(kernel_size, dtype=np.float32)\n",
        "    for i in range(kernel_size):\n",
        "        if i == M//2:\n",
        "            filter[i] = 2 * np.pi * cutoff\n",
        "        else:\n",
        "            filter[i] = (np.sin(2 * np.pi * cutoff * (i - M//2)) / (i - M//2)) * \\\n",
        "                    (0.42 - 0.5 * np.cos((2 * np.pi * i) / M) + 0.08 * np.cos(4 * np.pi * M))\n",
        "\n",
        "    filter = filter / np.sum(filter)\n",
        "    return filter\n",
        "\n",
        "class Resample1d(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, stride, transpose=False, padding=\"reflect\", trainable=False):\n",
        "        '''\n",
        "        Creates a resampling layer for time series data (using 1D convolution) - (N, C, W) input format\n",
        "        :param channels: Number of features C at each time-step\n",
        "        :param kernel_size: Width of sinc-based lowpass-filter (>= 15 recommended for good filtering performance)\n",
        "        :param stride: Resampling factor (integer)\n",
        "        :param transpose: False for down-, true for upsampling\n",
        "        :param padding: Either \"reflect\" to pad or \"valid\" to not pad\n",
        "        :param trainable: Optionally activate this to train the lowpass-filter, starting from the sinc initialisation\n",
        "        '''\n",
        "        super(Resample1d, self).__init__()\n",
        "\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.transpose = transpose\n",
        "        self.channels = channels\n",
        "\n",
        "        cutoff = 0.5 / stride\n",
        "\n",
        "        assert(kernel_size > 2)\n",
        "        assert ((kernel_size - 1) % 2 == 0)\n",
        "        assert(padding == \"reflect\" or padding == \"valid\")\n",
        "\n",
        "        filter = build_sinc_filter(kernel_size, cutoff)\n",
        "\n",
        "        self.filter = torch.nn.Parameter(torch.from_numpy(np.repeat(np.reshape(filter, [1, 1, kernel_size]), channels, axis=0)), requires_grad=trainable)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad here if not using transposed conv\n",
        "        input_size = x.shape[2]\n",
        "        if self.padding != \"valid\":\n",
        "            num_pad = (self.kernel_size-1)//2\n",
        "            out = F.pad(x, (num_pad, num_pad), mode=self.padding)\n",
        "        else:\n",
        "            out = x\n",
        "\n",
        "        # Lowpass filter (+ 0 insertion if transposed)\n",
        "        if self.transpose:\n",
        "            expected_steps = ((input_size - 1) * self.stride + 1)\n",
        "            if self.padding == \"valid\":\n",
        "                expected_steps = expected_steps - self.kernel_size + 1\n",
        "\n",
        "            out = F.conv_transpose1d(out, self.filter, stride=self.stride, padding=0, groups=self.channels)\n",
        "            diff_steps = out.shape[2] - expected_steps\n",
        "            if diff_steps > 0:\n",
        "                assert(diff_steps % 2 == 0)\n",
        "                out = out[:,:,diff_steps//2:-diff_steps//2]\n",
        "        else:\n",
        "            assert(input_size % self.stride == 1)\n",
        "            out = F.conv1d(out, self.filter, stride=self.stride, padding=0, groups=self.channels)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def get_output_size(self, input_size):\n",
        "        assert(input_size > 1)\n",
        "        if self.transpose:\n",
        "            if self.padding == \"valid\":\n",
        "                return ((input_size - 1) * self.stride + 1) - self.kernel_size + 1\n",
        "            else:\n",
        "                return ((input_size - 1) * self.stride + 1)\n",
        "        else:\n",
        "            assert(input_size % self.stride == 1) # Want to take first and last sample\n",
        "            if self.padding == \"valid\":\n",
        "                return input_size - self.kernel_size + 1\n",
        "            else:\n",
        "                return input_size\n",
        "\n",
        "    def get_input_size(self, output_size):\n",
        "        # Strided conv/decimation\n",
        "        if not self.transpose:\n",
        "            curr_size = (output_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
        "        else:\n",
        "            curr_size = output_size\n",
        "\n",
        "        # Conv\n",
        "        if self.padding == \"valid\":\n",
        "            curr_size = curr_size + self.kernel_size - 1 # o = i + p - k + 1\n",
        "\n",
        "        # Transposed\n",
        "        if self.transpose:\n",
        "            assert ((curr_size - 1) % self.stride == 0)# We need to have a value at the beginning and end\n",
        "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
        "        assert(curr_size > 0)\n",
        "        return curr_size\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, conv_type, transpose=False):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.transpose = transpose\n",
        "        self.stride = stride\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv_type = conv_type\n",
        "\n",
        "        # How many channels should be normalised as one group if GroupNorm is activated\n",
        "        # WARNING: Number of channels has to be divisible by this number!\n",
        "        NORM_CHANNELS = 8\n",
        "\n",
        "        if self.transpose:\n",
        "            self.filter = nn.ConvTranspose1d(n_inputs, n_outputs, self.kernel_size, stride, padding=kernel_size-1)\n",
        "        else:\n",
        "            self.filter = nn.Conv1d(n_inputs, n_outputs, self.kernel_size, stride)\n",
        "\n",
        "        if conv_type == \"gn\":\n",
        "            assert(n_outputs % NORM_CHANNELS == 0)\n",
        "            self.norm = nn.GroupNorm(n_outputs // NORM_CHANNELS, n_outputs)\n",
        "        elif conv_type == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(n_outputs, momentum=0.01)\n",
        "        # Add you own types of variations here!\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the convolution\n",
        "        if self.conv_type == \"gn\" or self.conv_type == \"bn\":\n",
        "            out = F.relu(self.norm((self.filter(x))))\n",
        "        else: # Add your own variations here with elifs conditioned on \"conv_type\" parameter!\n",
        "            assert(self.conv_type == \"normal\")\n",
        "            out = F.leaky_relu(self.filter(x))\n",
        "        return out\n",
        "\n",
        "    def get_input_size(self, output_size):\n",
        "        # Strided conv/decimation\n",
        "        if not self.transpose:\n",
        "            curr_size = (output_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
        "        else:\n",
        "            curr_size = output_size\n",
        "\n",
        "        # Conv\n",
        "        curr_size = curr_size + self.kernel_size - 1 # o = i + p - k + 1\n",
        "\n",
        "        # Transposed\n",
        "        if self.transpose:\n",
        "            assert ((curr_size - 1) % self.stride == 0)# We need to have a value at the beginning and end\n",
        "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
        "        assert(curr_size > 0)\n",
        "        return curr_size\n",
        "\n",
        "    def get_output_size(self, input_size):\n",
        "        # Transposed\n",
        "        if self.transpose:\n",
        "            assert(input_size > 1)\n",
        "            curr_size = (input_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
        "        else:\n",
        "            curr_size = input_size\n",
        "\n",
        "        # Conv\n",
        "        curr_size = curr_size - self.kernel_size + 1 # o = i + p - k + 1\n",
        "        assert (curr_size > 0)\n",
        "\n",
        "        # Strided conv/decimation\n",
        "        if not self.transpose:\n",
        "            assert ((curr_size - 1) % self.stride == 0)  # We need to have a value at the beginning and end\n",
        "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
        "\n",
        "        return curr_size"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCsiK2vO81E6",
        "colab_type": "text"
      },
      "source": [
        "### **Wave U-Net Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evUcitRz859M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UpsamplingBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
        "        super(UpsamplingBlock, self).__init__()\n",
        "        assert(stride > 1)\n",
        "\n",
        "        # CONV 1 for UPSAMPLING\n",
        "        if res == \"fixed\":\n",
        "            self.upconv = Resample1d(n_inputs, 15, stride, transpose=True)\n",
        "        else:\n",
        "            self.upconv = ConvLayer(n_inputs, n_inputs, kernel_size, stride, conv_type, transpose=True)\n",
        "\n",
        "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_outputs, kernel_size, 1, conv_type)] +\n",
        "                                                [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
        "\n",
        "        # CONVS to combine high- with low-level information (from shortcut)\n",
        "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_outputs + n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
        "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
        "\n",
        "    def forward(self, x, shortcut):\n",
        "        # UPSAMPLE HIGH-LEVEL FEATURES\n",
        "        upsampled = self.upconv(x)\n",
        "\n",
        "        for conv in self.pre_shortcut_convs:\n",
        "            upsampled = conv(upsampled)\n",
        "\n",
        "        # Prepare shortcut connection\n",
        "        combined = crop(shortcut, upsampled)\n",
        "\n",
        "        # Combine high- and low-level features\n",
        "        for conv in self.post_shortcut_convs:\n",
        "            combined = conv(torch.cat([combined, crop(upsampled, combined)], dim=1))\n",
        "        return combined\n",
        "\n",
        "    def get_output_size(self, input_size):\n",
        "        curr_size = self.upconv.get_output_size(input_size)\n",
        "\n",
        "        # Upsampling convs\n",
        "        for conv in self.pre_shortcut_convs:\n",
        "            curr_size = conv.get_output_size(curr_size)\n",
        "\n",
        "        # Combine convolutions\n",
        "        for conv in self.post_shortcut_convs:\n",
        "            curr_size = conv.get_output_size(curr_size)\n",
        "\n",
        "        return curr_size\n",
        "\n",
        "class DownsamplingBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
        "        super(DownsamplingBlock, self).__init__()\n",
        "        assert(stride > 1)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # CONV 1\n",
        "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_shortcut, kernel_size, 1, conv_type)] +\n",
        "                                                [ConvLayer(n_shortcut, n_shortcut, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
        "\n",
        "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
        "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in\n",
        "                                                  range(depth - 1)])\n",
        "\n",
        "        # CONV 2 with decimation\n",
        "        if res == \"fixed\":\n",
        "            self.downconv = Resample1d(n_outputs, 15, stride) # Resampling with fixed-size sinc lowpass filter\n",
        "        else:\n",
        "            self.downconv = ConvLayer(n_outputs, n_outputs, kernel_size, stride, conv_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # PREPARING SHORTCUT FEATURES\n",
        "        shortcut = x\n",
        "        for conv in self.pre_shortcut_convs:\n",
        "            shortcut = conv(shortcut)\n",
        "\n",
        "        # PREPARING FOR DOWNSAMPLING\n",
        "        out = shortcut\n",
        "        for conv in self.post_shortcut_convs:\n",
        "            out = conv(out)\n",
        "\n",
        "        # DOWNSAMPLING\n",
        "        out = self.downconv(out)\n",
        "\n",
        "        return out, shortcut\n",
        "\n",
        "    def get_input_size(self, output_size):\n",
        "        curr_size = self.downconv.get_input_size(output_size)\n",
        "\n",
        "        for conv in reversed(self.post_shortcut_convs):\n",
        "            curr_size = conv.get_input_size(curr_size)\n",
        "\n",
        "        for conv in reversed(self.pre_shortcut_convs):\n",
        "            curr_size = conv.get_input_size(curr_size)\n",
        "        return curr_size\n",
        "\n",
        "class Waveunet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, num_outputs, instruments, kernel_size, target_output_size, conv_type, res, separate=False, depth=1, strides=2):\n",
        "        super(Waveunet, self).__init__()\n",
        "\n",
        "        self.num_levels = len(num_channels)\n",
        "        self.strides = strides\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.depth = depth\n",
        "        self.instruments = instruments\n",
        "        self.separate = separate\n",
        "\n",
        "        # Only odd filter kernels allowed\n",
        "        assert(kernel_size % 2 == 1)\n",
        "\n",
        "        self.waveunets = nn.ModuleDict()\n",
        "\n",
        "        model_list = instruments if separate else [\"ALL\"]\n",
        "        # Create a model for each source if we separate sources separately, otherwise only one (model_list=[\"ALL\"])\n",
        "        for instrument in model_list:\n",
        "            module = nn.Module()\n",
        "\n",
        "            module.downsampling_blocks = nn.ModuleList()\n",
        "            module.upsampling_blocks = nn.ModuleList()\n",
        "\n",
        "            for i in range(self.num_levels - 1):\n",
        "                in_ch = num_inputs if i == 0 else num_channels[i]\n",
        "\n",
        "                module.downsampling_blocks.append(\n",
        "                    DownsamplingBlock(in_ch, num_channels[i], num_channels[i+1], kernel_size, strides, depth, conv_type, res))\n",
        "\n",
        "            for i in range(0, self.num_levels - 1):\n",
        "                module.upsampling_blocks.append(\n",
        "                    UpsamplingBlock(num_channels[-1-i], num_channels[-2-i], num_channels[-2-i], kernel_size, strides, depth, conv_type, res))\n",
        "\n",
        "            module.bottlenecks = nn.ModuleList(\n",
        "                [ConvLayer(num_channels[-1], num_channels[-1], kernel_size, 1, conv_type) for _ in range(depth)])\n",
        "\n",
        "            # Output conv\n",
        "            outputs = num_outputs if separate else num_outputs * len(instruments)\n",
        "            module.output_conv = nn.Conv1d(num_channels[0], outputs, 1)\n",
        "\n",
        "            self.waveunets[instrument] = module\n",
        "\n",
        "        self.set_output_size(target_output_size)\n",
        "\n",
        "    def set_output_size(self, target_output_size):\n",
        "        self.target_output_size = target_output_size\n",
        "\n",
        "        self.input_size, self.output_size = self.check_padding(target_output_size)\n",
        "        print(\"Using valid convolutions with \" + str(self.input_size) + \" inputs and \" + str(self.output_size) + \" outputs\")\n",
        "\n",
        "        assert((self.input_size - self.output_size) % 2 == 0)\n",
        "        self.shapes = {\"output_start_frame\" : (self.input_size - self.output_size) // 2,\n",
        "                       \"output_end_frame\" : (self.input_size - self.output_size) // 2 + self.output_size,\n",
        "                       \"output_frames\" : self.output_size,\n",
        "                       \"input_frames\" : self.input_size}\n",
        "\n",
        "    def check_padding(self, target_output_size):\n",
        "        # Ensure number of outputs covers a whole number of cycles so each output in the cycle is weighted equally during training\n",
        "        bottleneck = 1\n",
        "\n",
        "        while True:\n",
        "            out = self.check_padding_for_bottleneck(bottleneck, target_output_size)\n",
        "            if out is not False:\n",
        "                return out\n",
        "            bottleneck += 1\n",
        "\n",
        "    def check_padding_for_bottleneck(self, bottleneck, target_output_size):\n",
        "        module = self.waveunets[[k for k in self.waveunets.keys()][0]]\n",
        "        try:\n",
        "            curr_size = bottleneck\n",
        "            for idx, block in enumerate(module.upsampling_blocks):\n",
        "                curr_size = block.get_output_size(curr_size)\n",
        "            output_size = curr_size\n",
        "\n",
        "            # Bottleneck-Conv\n",
        "            curr_size = bottleneck\n",
        "            for block in reversed(module.bottlenecks):\n",
        "                curr_size = block.get_input_size(curr_size)\n",
        "            for idx, block in enumerate(reversed(module.downsampling_blocks)):\n",
        "                curr_size = block.get_input_size(curr_size)\n",
        "\n",
        "            assert(output_size >= target_output_size)\n",
        "            return curr_size, output_size\n",
        "        except AssertionError as e:\n",
        "            return False\n",
        "\n",
        "    def forward_module(self, x, module):\n",
        "        '''\n",
        "        A forward pass through a single Wave-U-Net (multiple Wave-U-Nets might be used, one for each source)\n",
        "        :param x: Input mix\n",
        "        :param module: Network module to be used for prediction\n",
        "        :return: Source estimates\n",
        "        '''\n",
        "        shortcuts = []\n",
        "        out = x\n",
        "\n",
        "        # DOWNSAMPLING BLOCKS\n",
        "        for block in module.downsampling_blocks:\n",
        "            out, short = block(out)\n",
        "            shortcuts.append(short)\n",
        "\n",
        "        # BOTTLENECK CONVOLUTION\n",
        "        for conv in module.bottlenecks:\n",
        "            out = conv(out)\n",
        "\n",
        "        # UPSAMPLING BLOCKS\n",
        "        for idx, block in enumerate(module.upsampling_blocks):\n",
        "            out = block(out, shortcuts[-1 - idx])\n",
        "\n",
        "        # OUTPUT CONV\n",
        "        out = module.output_conv(out)\n",
        "        if not self.training:  # At test time clip predictions to valid amplitude range\n",
        "            out = out.clamp(min=-1.0, max=1.0)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, inst=None):\n",
        "        curr_input_size = x.shape[-1]\n",
        "        assert(curr_input_size == self.input_size) # User promises to feed the proper input himself, to get the pre-calculated (NOT the originally desired) output size\n",
        "\n",
        "        if self.separate:\n",
        "            return {inst : self.forward_module(x, self.waveunets[inst])}\n",
        "        else:\n",
        "            assert(len(self.waveunets) == 1)\n",
        "            out = self.forward_module(x, self.waveunets[\"ALL\"])\n",
        "\n",
        "            out_dict = {}\n",
        "            for idx, inst in enumerate(self.instruments):\n",
        "                out_dict[inst] = out[:, idx * self.num_outputs:(idx + 1) * self.num_outputs]\n",
        "            return out_dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BusvAI7w86df",
        "colab_type": "text"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9jmyaSH9CR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb33c72e-e537-4f5e-cf73-259550a170c5"
      },
      "source": [
        "num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
        "               [args.features*2**i for i in range(0, args.levels)]\n",
        "\n",
        "target_outputs = int(args.output_size * args.sr)\n",
        "\n",
        "model = Waveunet(args.channels, num_features, args.channels,args.instruments , kernel_size=args.kernel_size,\n",
        "                 target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
        "                 conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
        "\n",
        "\n",
        "print('model: ', model)\n",
        "print('parameter count: ', str(sum(p.numel() for p in model.parameters())))\n",
        "\n",
        "writer = SummaryWriter(args.log_dir)\n",
        "musdb = get_musdb_folds(args.dataset_dir)\n",
        "\n",
        "# If not data augmentation, at least crop targets to fit model output shape\n",
        "crop_func = partial(crop_sep, shapes=model.shapes)\n",
        "\n",
        "# Data augmentation function for training\n",
        "augment_func = partial(random_amplify, shapes=model.shapes, min=0.7, max=1.0)\n",
        "train_data = SeparationDataset(musdb, \"train\", args.instruments, args.sr, args.channels, model.shapes, True, args.hdf_dir, audio_transform=augment_func)\n",
        "val_data = SeparationDataset(musdb, \"val\", args.instruments, args.sr, args.channels, model.shapes, False, args.hdf_dir, audio_transform=crop_func)\n",
        "test_data = SeparationDataset(musdb, \"test\", args.instruments, args.sr, args.channels, model.shapes, False, args.hdf_dir, audio_transform=crop_func)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, worker_init_fn=worker_init_fn)\n",
        "\n",
        "if args.loss == \"L1\":\n",
        "    criterion = nn.L1Loss()\n",
        "elif args.loss == \"L2\":\n",
        "    criterion = nn.MSELoss()\n",
        "else:\n",
        "    raise NotImplementedError(\"Couldn't find this loss!\")\n",
        "\n",
        "optimizer = Adam(params=model.parameters(), lr=args.lr)\n",
        "\n",
        "# Set up training state dict that will also be saved into checkpoints\n",
        "state = {\"step\" : 0,\n",
        "          \"worse_epochs\" : 0,\n",
        "          \"epochs\" : 0,\n",
        "          \"best_loss\" : np.Inf}\n",
        "\n",
        "\n",
        "print('TRAINING START')\n",
        "while state[\"worse_epochs\"] < args.patience:\n",
        "        print(\"Training one epoch from iteration \" + str(state[\"step\"]))\n",
        "        avg_time = 0.\n",
        "        model.train()\n",
        "        with tqdm(total=len(train_data) // args.batch_size) as pbar:\n",
        "            np.random.seed()\n",
        "            for example_num, (x, targets) in enumerate(dataloader):\n",
        "                t = time.time()\n",
        "\n",
        "                # Set LR for this iteration\n",
        "                set_cyclic_lr(optimizer, example_num, len(train_data) // args.batch_size, args.cycles, args.min_lr, args.lr)\n",
        "                writer.add_scalar(\"lr\", get_lr(optimizer), state[\"step\"])\n",
        "\n",
        "                # Compute loss for each instrument/model\n",
        "                optimizer.zero_grad()\n",
        "                outputs, avg_loss = compute_loss(model, x, targets, criterion, compute_grad=True)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                t = time.time() - t\n",
        "                avg_time += (1. / float(example_num + 1)) * (t - avg_time)\n",
        "\n",
        "                writer.add_scalar(\"train_loss\", avg_loss, state[\"step\"])\n",
        "\n",
        "                if example_num % args.example_freq == 0:\n",
        "                    input_centre = torch.mean(x[0, :, model.shapes[\"output_start_frame\"]:model.shapes[\"output_end_frame\"]], 0) # Stereo not supported for logs yet\n",
        "                    writer.add_audio(\"input\", input_centre, state[\"step\"], sample_rate=args.sr)\n",
        "\n",
        "                    for inst in outputs.keys():\n",
        "                        writer.add_audio(inst + \"_pred\", torch.mean(outputs[inst][0], 0), state[\"step\"], sample_rate=args.sr)\n",
        "                        writer.add_audio(inst + \"_target\", torch.mean(targets[inst][0], 0), state[\"step\"], sample_rate=args.sr)\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        # VALIDATE\n",
        "        val_loss = validate(args, model, criterion, val_data)\n",
        "        print(\"VALIDATION FINISHED: LOSS: \" + str(val_loss))\n",
        "        writer.add_scalar(\"val_loss\", val_loss, state[\"step\"])\n",
        "\n",
        "        # EARLY STOPPING CHECK\n",
        "        checkpoint_path = os.path.join(args.checkpoint_dir, \"checkpoint_\" + str(state[\"step\"]))\n",
        "        if val_loss >= state[\"best_loss\"]:\n",
        "            state[\"worse_epochs\"] += 1\n",
        "        else:\n",
        "            print(\"MODEL IMPROVED ON VALIDATION SET!\")\n",
        "            state[\"worse_epochs\"] = 0\n",
        "            state[\"best_loss\"] = val_loss\n",
        "            state[\"best_checkpoint\"] = checkpoint_path\n",
        "\n",
        "        state[\"epochs\"] += 1\n",
        "\n",
        "              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using valid convolutions with 97961 inputs and 88409 outputs\n",
            "model:  Waveunet(\n",
            "  (waveunets): ModuleDict(\n",
            "    (bass): Module(\n",
            "      (downsampling_blocks): ModuleList(\n",
            "        (0): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (1): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (2): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (3): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (4): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "      )\n",
            "      (upsampling_blocks): ModuleList(\n",
            "        (0): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (4): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (bottlenecks): ModuleList(\n",
            "        (0): ConvLayer(\n",
            "          (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
            "          (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (drums): Module(\n",
            "      (downsampling_blocks): ModuleList(\n",
            "        (0): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (1): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (2): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (3): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (4): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "      )\n",
            "      (upsampling_blocks): ModuleList(\n",
            "        (0): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (4): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (bottlenecks): ModuleList(\n",
            "        (0): ConvLayer(\n",
            "          (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
            "          (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (others): Module(\n",
            "      (downsampling_blocks): ModuleList(\n",
            "        (0): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (1): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (2): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (3): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (4): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "      )\n",
            "      (upsampling_blocks): ModuleList(\n",
            "        (0): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (4): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (bottlenecks): ModuleList(\n",
            "        (0): ConvLayer(\n",
            "          (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
            "          (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (vocals): Module(\n",
            "      (downsampling_blocks): ModuleList(\n",
            "        (0): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (1): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (2): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (3): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "        (4): DownsamplingBlock(\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (downconv): Resample1d()\n",
            "        )\n",
            "      )\n",
            "      (upsampling_blocks): ModuleList(\n",
            "        (0): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (4): UpsamplingBlock(\n",
            "          (upconv): Resample1d()\n",
            "          (pre_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "          (post_shortcut_convs): ModuleList(\n",
            "            (0): ConvLayer(\n",
            "              (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
            "              (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (bottlenecks): ModuleList(\n",
            "        (0): ConvLayer(\n",
            "          (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
            "          (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "parameter count:  70148232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2163 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAINING START\n",
            "Training one epoch from iteration 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 69/2163 [1:12:14<35:25:01, 60.89s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uat_mv1ttAz5",
        "colab_type": "text"
      },
      "source": [
        "# **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkJzk1KetCPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(audio, model):\n",
        "    if isinstance(audio, torch.Tensor):\n",
        "        is_cuda = audio.is_cuda()\n",
        "        audio = audio.detach().cpu().numpy()\n",
        "        return_mode = \"pytorch\"\n",
        "    else:\n",
        "        return_mode = \"numpy\"\n",
        "\n",
        "    expected_outputs = audio.shape[1]\n",
        "\n",
        "    # Pad input if it is not divisible in length by the frame shift number\n",
        "    output_shift = model.shapes[\"output_frames\"]\n",
        "    pad_back = audio.shape[1] % output_shift\n",
        "    pad_back = 0 if pad_back == 0 else output_shift - pad_back\n",
        "    if pad_back > 0:\n",
        "        audio = np.pad(audio, [(0,0), (0, pad_back)], mode=\"constant\", constant_values=0.0)\n",
        "\n",
        "    target_outputs = audio.shape[1]\n",
        "    outputs = {key: np.zeros(audio.shape, np.float32) for key in model.instruments}\n",
        "\n",
        "    # Pad mixture across time at beginning and end so that neural network can make prediction at the beginning and end of signal\n",
        "    pad_front_context = model.shapes[\"output_start_frame\"]\n",
        "    pad_back_context = model.shapes[\"input_frames\"] - model.shapes[\"output_end_frame\"]\n",
        "    audio = np.pad(audio, [(0,0), (pad_front_context, pad_back_context)], mode=\"constant\", constant_values=0.0)\n",
        "\n",
        "    # Iterate over mixture magnitudes, fetch network prediction\n",
        "    with torch.no_grad():\n",
        "        for target_start_pos in range(0, target_outputs, model.shapes[\"output_frames\"]):\n",
        "\n",
        "            # Prepare mixture excerpt by selecting time interval\n",
        "            curr_input = audio[:, target_start_pos:target_start_pos + model.shapes[\"input_frames\"]] # Since audio was front-padded input of [targetpos:targetpos+inputframes] actually predicts [targetpos:targetpos+outputframes] target range\n",
        "\n",
        "            # Convert to Pytorch tensor for model prediction\n",
        "            curr_input = torch.from_numpy(curr_input).unsqueeze(0)\n",
        "\n",
        "            # Predict\n",
        "            for key, curr_targets in compute_output(model, curr_input).items():\n",
        "                outputs[key][:,target_start_pos:target_start_pos+model.shapes[\"output_frames\"]] = curr_targets.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Crop to expected length (since we padded to handle the frame shift)\n",
        "    outputs = {key : outputs[key][:,:expected_outputs] for key in outputs.keys()}\n",
        "\n",
        "    if return_mode == \"pytorch\":\n",
        "        outputs = torch.from_numpy(outputs)\n",
        "        if is_cuda:\n",
        "            outputs = outputs.cuda()\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMriQmO7t6GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_song(args, audio_path, model):\n",
        "    model.eval()\n",
        "\n",
        "    # Load mixture in original sampling rate\n",
        "    mix_audio, mix_sr = load(audio_path, sr=None, mono=False)\n",
        "    mix_channels = mix_audio.shape[0]\n",
        "    mix_len = mix_audio.shape[1]\n",
        "\n",
        "    # Adapt mixture channels to required input channels\n",
        "    if args.channels == 1:\n",
        "        mix_audio = np.mean(mix_audio, axis=0, keepdims=True)\n",
        "    else:\n",
        "        if mix_channels == 1: # Duplicate channels if input is mono but model is stereo\n",
        "            mix_audio = np.tile(mix_audio, [args.channels, 1])\n",
        "        else:\n",
        "            assert(mix_channels == args.channels)\n",
        "\n",
        "    # resample to model sampling rate\n",
        "    mix_audio = resample(mix_audio, mix_sr, args.sr)\n",
        "\n",
        "    sources = predict(mix_audio, model)\n",
        "\n",
        "    # Resample back to mixture sampling rate in case we had model on different sampling rate\n",
        "    sources = {key : resample(sources[key], args.sr, mix_sr) for key in sources.keys()}\n",
        "\n",
        "    # In case we had to pad the mixture at the end, or we have a few samples too many due to inconsistent down- and upsamṕling, remove those samples from source prediction now\n",
        "    for key in sources.keys():\n",
        "        diff = sources[key].shape[1] - mix_len\n",
        "        if diff > 0:\n",
        "            print(\"WARNING: Cropping \" + str(diff) + \" samples\")\n",
        "            sources[key] = sources[key][:, :-diff]\n",
        "        elif diff < 0:\n",
        "            print(\"WARNING: Padding output by \" + str(diff) + \" samples\")\n",
        "            sources[key] = np.pad(sources[key], [(0,0), (0, -diff)], \"constant\", 0.0)\n",
        "\n",
        "        # Adapt channels\n",
        "        if mix_channels > args.channels:\n",
        "            assert(args.channels == 1)\n",
        "            # Duplicate mono predictions\n",
        "            sources[key] = np.tile(sources[key], [mix_channels, 1])\n",
        "        elif mix_channels < args.channels:\n",
        "            assert(mix_channels == 1)\n",
        "            # Reduce model output to mono\n",
        "            sources[key] = np.mean(sources[key], axis=0, keepdims=True)\n",
        "\n",
        "        sources[key] = np.asfortranarray(sources[key]) # So librosa does not complain if we want to save it\n",
        "\n",
        "    return sources\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WiQiX7buD51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(args, dataset, model, instruments):\n",
        "    perfs = list()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for example in dataset:\n",
        "            print(\"Evaluating \" + example[\"mix\"])\n",
        "\n",
        "            # Load source references in their original sr and channel number\n",
        "            target_sources = np.stack([load(example[instrument], sr=None, mono=False)[0].T for instrument in instruments])\n",
        "\n",
        "            # Predict using mixture\n",
        "            pred_sources = predict_song(args, example[\"mix\"], model)\n",
        "            pred_sources = np.stack([pred_sources[key].T for key in instruments])\n",
        "\n",
        "            # Evaluate\n",
        "            SDR, ISR, SIR, SAR, _ = museval.metrics.bss_eval(target_sources, pred_sources)\n",
        "            song = {}\n",
        "            for idx, name in enumerate(instruments):\n",
        "                song[name] = {\"SDR\" : SDR[idx], \"ISR\" : ISR[idx], \"SIR\" : SIR[idx], \"SAR\" : SAR[idx]}\n",
        "            perfs.append(song)\n",
        "\n",
        "    return perfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biFszomhuEsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(args, model, criterion, test_data):\n",
        "    # PREPARE DATA\n",
        "    dataloader = torch.utils.data.DataLoader(test_data,\n",
        "                                             batch_size=args.batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=args.num_workers)\n",
        "\n",
        "    # VALIDATE\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with tqdm(total=len(test_data) // args.batch_size) as pbar, torch.no_grad():\n",
        "        for example_num, (x, targets) in enumerate(dataloader):\n",
        "            if args.cuda:\n",
        "                x = x.cuda()\n",
        "                for k in list(targets.keys()):\n",
        "                    targets[k] = targets[k].cuda()\n",
        "\n",
        "            _, avg_loss = compute_loss(model, x, targets, criterion)\n",
        "\n",
        "            total_loss += (1. / float(example_num + 1)) * (avg_loss - total_loss)\n",
        "\n",
        "            pbar.set_description(\"Current loss: {:.4f}\".format(total_loss))\n",
        "            pbar.update(1)\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEdXQbgOs3Q9",
        "colab_type": "text"
      },
      "source": [
        "# **Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28qQvlu9uaw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pred_Args:\n",
        "  instruments = ['bass', 'drums', 'other', 'vocals']\n",
        "  features = 32\n",
        "  load_model = '/content/drive/My Drive/BTP/Dataset/checkpoints/models/waveunet/model'\n",
        "  batch_size = 4\n",
        "  levels = 6\n",
        "  depth = 1\n",
        "  sr = 44100\n",
        "  channels = 2\n",
        "  kernel_size = 5\n",
        "  output_size = 2.0\n",
        "  strides = 4\n",
        "  conv_type = 'gn'\n",
        "  res = 'fixed'\n",
        "  separate = 1\n",
        "  feature_growth = 'double'\n",
        "  input = '/content/drive/My Drive/BTP/Dataset/input/predict.wav'\n",
        "  output = '/content/drive/My Drive/BTP/Dataset/outputs'\n",
        "\n",
        "args = Pred_Args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La1T_C0avTyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(args):\n",
        "    num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
        "                   [args.features*2**i for i in range(0, args.levels)]\n",
        "    target_outputs = int(args.output_size * args.sr)\n",
        "    model = Waveunet(args.channels, num_features, args.channels, args.instruments, kernel_size=args.kernel_size,\n",
        "                     target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
        "                     conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
        "\n",
        "    print(\"Loading model from checkpoint \" + str(args.load_model))\n",
        "    state = load_model(model, None, args.load_model)\n",
        "    print('Step', state['step'])\n",
        "\n",
        "    preds = predict_song(args, args.input, model)\n",
        "\n",
        "    output_folder = args.output\n",
        "    for inst in preds.keys():\n",
        "        write_wav(os.path.join(output_folder, os.path.basename(args.input) + \"_\" + inst + \".wav\"), preds[inst], args.sr)\n",
        "    print(\"The song has been split into vocals, bass, drum, others. Check '/content/drive/My Drive/BTP/Dataset/outputs' \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMH226NUs6jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main(args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}