{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnetAudioSeparator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONKwaT3NWMjEr2vwzhLyK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreya0505/MusicalSourceSeparation/blob/master/UnetAudioSeparator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHmt_LTQIam",
        "colab_type": "text"
      },
      "source": [
        "# **Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmN5he8RuPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe659b6f-70b3-4f41-f7fd-e45278a4e15a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12GJqQ64CfFs",
        "colab_type": "text"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izWNEwZl4KuM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "4f8fb182-81a6-4f98-f6dc-678dfe998078"
      },
      "source": [
        "pip install tensorflow==1.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.8 in /usr/local/lib/python3.6/dist-packages (1.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.29.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.18.5)\n",
            "Requirement already satisfied: tensorboard<1.9.0,>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8) (47.3.1)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (1.5.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (1.0.1)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (0.9999999)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIk9JBKU3CGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "6adef804-35cf-4c4d-ebb1-e8ea3bfcc192"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmYJAzRWWlV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31f7e59d-0158-4bf0-d071-d600a99d0940"
      },
      "source": [
        "pip install FFProbe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: FFProbe in /usr/local/lib/python3.6/dist-packages (0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsASoKr8CkcU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f821c4db-b12d-4207-c08e-b464d90170a9"
      },
      "source": [
        "pip install ffmpeg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.6/dist-packages (1.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsEVwReVComW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb9d1e74-bf14-4f36-ca7b-a6b1cd09c2a9"
      },
      "source": [
        "pip install ffprobe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ffprobe in /usr/local/lib/python3.6/dist-packages (0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUvVThx5Cqqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4ce36bad-bc80-4f34-93ac-c115c12b7d5a"
      },
      "source": [
        "pip install stempeg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stempeg in /usr/local/lib/python3.6/dist-packages (0.1.8)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from stempeg) (0.10.3.post1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from stempeg) (1.18.5)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->stempeg) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->stempeg) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7NGKC-mFIq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6af19f13-a94a-4229-bed7-53dbb2287718"
      },
      "source": [
        "pip install musdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: musdb in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: pyaml in /usr/local/lib/python3.6/dist-packages (from musdb) (20.4.0)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from musdb) (0.10.3.post1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from musdb) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from musdb) (4.41.1)\n",
            "Requirement already satisfied: stempeg>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from musdb) (0.1.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml->musdb) (3.13)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->musdb) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->musdb) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1bOhY6oFPj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "adda7e3d-d0fa-49a7-adfa-1382e5d67bc7"
      },
      "source": [
        "pip install soundfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxS2oDbdEjz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob2 as glob\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "from multiprocessing import Process\n",
        "from lxml import etree\n",
        "import soundfile as sf\n",
        "import musdb\n",
        "import functools\n",
        "import csv \n",
        "import ffmpeg\n",
        "import stempeg\n",
        "from tensorflow.contrib.signal.python.ops import window_ops\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkOjgTOlC5Az",
        "colab_type": "text"
      },
      "source": [
        "# **Config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv_sUiqiCvsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model_config = {\"musdb_path\" : \"/content/drive/My Drive/BTP/Dataset\", # SET MUSDB PATH HERE, AND SET CCMIXTER PATH IN CCMixter.xml\n",
        "                 \"directory\" : \"/content/drive/My Drive/BTP/Dataset/partitions\",\n",
        "                    \"estimates_path\" : \"/content/drive/My Drive/BTP/Dataset/source_estimates\", # SET THIS PATH TO WHERE YOU WANT SOURCE ESTIMATES PRODUCED BY THE TRAINED MODEL TO BE SAVED. Folder itself must exist!\n",
        "                    \"data_path\" : \"/content/drive/My Drive/BTP/Dataset/data\", # Set this to where the preprocessed dataset should be saved\n",
        "                    \"model_base_dir\" : \"/content/drive/My Drive/BTP/Dataset/checkpoints\", # Base folder for model checkpoints\n",
        "                    \"log_dir\" : \"/content/drive/My Drive/BTP/Dataset/logs\", # Base folder for logs files\n",
        "                    \"batch_size\" : 16, # Batch size\n",
        "                    \"init_sup_sep_lr\" : 1e-4, # Supervised separator learning rate\n",
        "                    \"epoch_it\" : 0, # Number of supervised separator steps per epoch\n",
        "                    'cache_size': 4000, # Number of audio snippets buffered in the random shuffle queue. Larger is better, since workers put multiple examples of one song into this queue. The number of different songs that is sampled from with each batch equals cache_size / num_snippets_per_track. Set as high as your RAM allows.\n",
        "                    'num_workers' : 4, # Number of processes used for each TF map operation used when loading the dataset\n",
        "                    \"num_snippets_per_track\" : 100, # Number of snippets that should be extracted from each song at a time after loading it. Higher values make data loading faster, but can reduce the batches song diversity\n",
        "                    'num_layers' : 12, # How many U-Net layers\n",
        "                    'filter_size' : 15, # For Wave-U-Net: Filter size of conv in downsampling block\n",
        "                    'merge_filter_size' : 5, # For Wave-U-Net: Filter size of conv in upsampling block\n",
        "                    'input_filter_size' : 15, # For Wave-U-Net: Filter size of first convolution in first downsampling block\n",
        "                    'output_filter_size': 1, # For Wave-U-Net: Filter size of convolution in the output layer\n",
        "                    'num_initial_filters' : 24, # Number of filters for convolution in first layer of network\n",
        "                    \"num_frames\": 16384, # DESIRED number of time frames in the output waveform per samples (could be changed when using valid padding)\n",
        "                    'expected_sr': 22050,  # Downsample all audio input to this sampling rate\n",
        "                    'mono_downmix': True,  # Whether to downsample the audio input\n",
        "                    'output_type' : 'direct', # Type of output layer, either \"direct\" or \"difference\". Direct output: Each source is result of tanh activation and independent. DIfference: Last source output is equal to mixture input - sum(all other sources)\n",
        "                    'output_activation' : 'tanh', # Activation function for output layer. \"tanh\" or \"linear\". Linear output involves clipping to [-1,1] at test time, and might be more stable than tanh\n",
        "                    'context' : False, # Type of padding for convolutions in separator. If False, feature maps double or half in dimensions after each convolution, and convolutions are padded with zeros (\"same\" padding). If True, convolution is only performed on the available mixture input, thus the output is smaller than the input\n",
        "                    'network' : 'unet', # Type of network architecture, either unet (our model) or unet_spectrogram (Jansson et al 2017 model)\n",
        "                    'upsampling' : 'linear', # Type of technique used for upsampling the feature maps in a unet architecture, either 'linear' interpolation or 'learned' filling in of extra samples\n",
        "                    'task' : 'voice', # Type of separation task. 'voice' : Separate music into voice and accompaniment. 'multi_instrument': Separate music into guitar, bass, vocals, drums and other (Sisec)\n",
        "                    'augmentation' : True, # Random attenuation of source signals to improve generalisation performance (data augmentation)\n",
        "                    'raw_audio_loss' : True, # Only active for unet_spectrogram network. True: L2 loss on audio. False: L1 loss on spectrogram magnitudes for training and validation and test loss\n",
        "                    'worse_epochs' : 20, # Patience for early stoppping on validation set\n",
        "                    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBngnyLODPcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "experiment_id = np.random.randint(0,1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUEL_eJkDVah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline():\n",
        "    print(\"Training baseline model\")\n",
        "\n",
        "def baseline_diff():\n",
        "    print(\"Training baseline model with difference output\")\n",
        "    model_config = {\n",
        "        \"output_type\" : \"difference\"\n",
        "    }\n",
        "def baseline_context():\n",
        "    print(\"Training baseline model with difference output and input context (valid convolutions)\")\n",
        "    model_config = {\n",
        "        \"output_type\" : \"difference\",\n",
        "        \"context\" : True\n",
        "    }\n",
        "\n",
        "def baseline_stereo():\n",
        "    print(\"Training baseline model with difference output and input context (valid convolutions) and stereo input/output\")\n",
        "    model_config = {\n",
        "        \"output_type\" : \"difference\",\n",
        "        \"context\" : True,\n",
        "        \"mono_downmix\" : False\n",
        "    }\n",
        "def full():\n",
        "    print(\"Training full singing voice separation model, with difference output and input context (valid convolutions) and stereo input/output, and learned upsampling layer\")\n",
        "    model_config = {\n",
        "        \"output_type\" : \"difference\",\n",
        "        \"context\" : True,\n",
        "        \"upsampling\": \"learned\",\n",
        "        \"mono_downmix\" : False\n",
        "    }\n",
        "def full_44KHz():\n",
        "    print(\"Training full singing voice separation model, with difference output and input context (valid convolutions) and stereo input/output, and learned upsampling layer, and 44.1 KHz sampling rate\")\n",
        "    model_config = {\n",
        "        \"output_type\" : \"difference\",\n",
        "        \"context\" : True,\n",
        "        \"upsampling\": \"learned\",\n",
        "        \"mono_downmix\" : False,\n",
        "        \"expected_sr\" : 44100\n",
        "    }\n",
        "def baseline_context_smallfilter_deep():\n",
        "    model_config = {\n",
        "        \"output_type\": \"difference\",\n",
        "        \"context\": True,\n",
        "        \"num_layers\" : 14,\n",
        "        \"duration\" : 7,\n",
        "        \"filter_size\" : 5,\n",
        "        \"merge_filter_size\" : 1\n",
        "    }\n",
        "\n",
        "def full_multi_instrument():\n",
        "    print(\"Training multi-instrument separation with best model\")\n",
        "    model_config = {\n",
        "        \"output_type\": \"difference\",\n",
        "        \"context\": True,\n",
        "        \"upsampling\": \"linear\",\n",
        "        \"mono_downmix\": False,\n",
        "        \"task\" : \"multi_instrument\"\n",
        "    }\n",
        "\n",
        "def baseline_comparison():\n",
        "    model_config = {\n",
        "        \"batch_size\": 4, # Less output since model is so big. Doesn't matter since the model's output is not dependent on its output or input size (only convolutions)\n",
        "\n",
        "        \"output_type\": \"difference\",\n",
        "        \"context\": True,\n",
        "        \"num_frames\" : 768*127 + 1024,\n",
        "        \"duration\" : 13,\n",
        "        \"expected_sr\" : 8192,\n",
        "        \"num_initial_filters\" : 34\n",
        "    }\n",
        "\n",
        "def unet_spectrogram():\n",
        "    model_config = {\n",
        "        \"batch_size\": 4, # Less output since model is so big.\n",
        "\n",
        "        \"network\" : \"unet_spectrogram\",\n",
        "        \"num_layers\" : 6,\n",
        "        \"expected_sr\" : 8192,\n",
        "        \"num_frames\" : 768 * 127 + 1024, # hop_size * (time_frames_of_spectrogram_input - 1) + fft_length\n",
        "        \"duration\" : 13,\n",
        "        \"num_initial_filters\" : 16\n",
        "    }\n",
        "def unet_spectrogram_l1():\n",
        "    model_config = {\n",
        "        \"batch_size\": 4, # Less output since model is so big.\n",
        "\n",
        "        \"network\" : \"unet_spectrogram\",\n",
        "        \"num_layers\" : 6,\n",
        "        \"expected_sr\" : 8192,\n",
        "        \"num_frames\" : 768 * 127 + 1024, # hop_size * (time_frames_of_spectrogram_input - 1) + fft_length\n",
        "        \"duration\" : 13,\n",
        "        \"num_initial_filters\" : 16,\n",
        "        \"raw_audio_loss\" : False\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjYAIgyQEwHS",
        "colab_type": "text"
      },
      "source": [
        "# **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHfvNH_e0bQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTrainableVariables(tag=\"\"):\n",
        "    return [v for v in tf.trainable_variables() if tag in v.name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeEzVm140dEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNumParams(tensors):\n",
        "    return np.sum([np.prod(t.get_shape().as_list()) for t in tensors])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdoVlM0Ue4NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_and_concat(x1,x2, match_feature_dim=True):\n",
        "    '''\n",
        "    Copy-and-crop operation for two feature maps of different size.\n",
        "    Crops the first input x1 equally along its borders so that its shape is equal to \n",
        "    the shape of the second input x2, then concatenates them along the feature channel axis.\n",
        "    :param x1: First input that is cropped and combined with the second input\n",
        "    :param x2: Second input\n",
        "    :return: Combined feature map\n",
        "    '''\n",
        "    if x2 is None:\n",
        "        return x1\n",
        "\n",
        "    x1 = crop(x1,x2.get_shape().as_list(), match_feature_dim)\n",
        "    return tf.concat([x1, x2], axis=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8yHbPxgDsrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_amplify(sample):\n",
        "    '''\n",
        "    Randomly amplifies or attenuates the input signal\n",
        "    :return: Amplified signal\n",
        "    '''\n",
        "    for key, val in list(sample.items()):\n",
        "        if key != \"mix\":\n",
        "            sample[key] = tf.random_uniform([], 0.7, 1.0) * val\n",
        "\n",
        "    sample[\"mix\"] = tf.add_n([val for key, val in list(sample.items()) if key != \"mix\"])\n",
        "    return sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhvaOa7RWJV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_sample(sample, crop_frames):\n",
        "    for key, val in list(sample.items()):\n",
        "        if key != \"mix\" and crop_frames > 0:\n",
        "            sample[key] = val[crop_frames:-crop_frames,:]\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLDXAgQq0mhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_freqs(tensor, target_shape):\n",
        "    '''\n",
        "    Pads the frequency axis of a 4D tensor of shape [batch_size, freqs, timeframes, channels] or 2D tensor [freqs, timeframes] with zeros\n",
        "    so that it reaches the target shape. If the number of frequencies to pad is uneven, the rows are appended at the end. \n",
        "    :param tensor: Input tensor to pad with zeros along the frequency axis\n",
        "    :param target_shape: Shape of tensor after zero-padding\n",
        "    :return: Padded tensor\n",
        "    '''\n",
        "    target_freqs = (target_shape[1] if len(target_shape) == 4 else target_shape[0]) #TODO\n",
        "    if isinstance(tensor, tf.Tensor):\n",
        "        input_shape = tensor.get_shape().as_list()\n",
        "    else:\n",
        "        input_shape = tensor.shape\n",
        "\n",
        "    if len(input_shape) == 2:\n",
        "        input_freqs = input_shape[0]\n",
        "    else:\n",
        "        input_freqs = input_shape[1]\n",
        "\n",
        "    diff = target_freqs - input_freqs\n",
        "    if diff % 2 == 0:\n",
        "        pad = [(diff/2, diff/2)]\n",
        "    else:\n",
        "        pad = [(diff//2, diff//2 + 1)] # Add extra frequency bin at the end\n",
        "\n",
        "    if len(target_shape) == 2:\n",
        "        pad = pad + [(0,0)]\n",
        "    else:\n",
        "        pad = [(0,0)] + pad + [(0,0), (0,0)]\n",
        "\n",
        "    if isinstance(tensor, tf.Tensor):\n",
        "        return tf.pad(tensor, pad, mode='constant', constant_values=0.0)\n",
        "    else:\n",
        "        return np.pad(tensor, pad, mode='constant', constant_values=0.0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn5NiXLr0ppB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LeakyReLU(x, alpha=0.2):\n",
        "    return tf.maximum(alpha*x, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2zO1LV0rJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AudioClip(x, training):\n",
        "    '''\n",
        "    Simply returns the input if training is set to True, otherwise clips the input to [-1,1]\n",
        "    :param x: Input tensor (coming from last layer of neural network)\n",
        "    :param training: Whether model is in training (True) or testing mode (False)\n",
        "    :return: Output tensor (potentially clipped)\n",
        "    '''\n",
        "    if training:\n",
        "        return x\n",
        "    else:\n",
        "        return tf.maximum(tf.minimum(x, 1.0), -1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9dtsfiP0tce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resample(audio, orig_sr, new_sr):\n",
        "    return librosa.resample(audio.T, orig_sr, new_sr).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6mDb2Dd0wAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load(path, sr=22050, mono=True, offset=0.0, duration=None, dtype=np.float32):\n",
        "    # ALWAYS output (n_frames, n_channels) audio\n",
        "    y, orig_sr = librosa.load(path, sr, mono, offset, duration, dtype)\n",
        "    if len(y.shape) == 1:\n",
        "        y = np.expand_dims(y, axis=0)\n",
        "    return y.T, orig_sr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlIORC_E0ycG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop(tensor, target_shape, match_feature_dim=True):\n",
        "    '''\n",
        "    Crops a 3D tensor [batch_size, width, channels] along the width axes to a target shape.\n",
        "    Performs a centre crop. If the dimension difference is uneven, crop last dimensions first.\n",
        "    :param tensor: 4D tensor [batch_size, width, height, channels] that should be cropped. \n",
        "    :param target_shape: Target shape (4D tensor) that the tensor should be cropped to\n",
        "    :return: Cropped tensor\n",
        "    '''\n",
        "    shape = np.array(tensor.get_shape().as_list())\n",
        "    diff = shape - np.array(target_shape)\n",
        "    assert(diff[0] == 0 and (diff[2] == 0 or not match_feature_dim))# Only width axis can differ\n",
        "    if (diff[1] % 2 != 0):\n",
        "        print(\"WARNING: Cropping with uneven number of extra entries on one side\")\n",
        "    assert diff[1] >= 0 # Only positive difference allowed\n",
        "    if diff[1] == 0:\n",
        "        return tensor\n",
        "    crop_start = diff // 2\n",
        "    crop_end = diff - crop_start\n",
        "\n",
        "    return tensor[:,crop_start[1]:-crop_end[1],:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5YLQnUa02I8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spectrogramToAudioFile(magnitude, fftWindowSize, hopSize, phaseIterations=10, phase=None, length=None):\n",
        "    '''\n",
        "    Computes an audio signal from the given magnitude spectrogram, and optionally an initial phase.\n",
        "    Griffin-Lim is executed to recover/refine the given the phase from the magnitude spectrogram.\n",
        "    :param magnitude: Magnitudes to be converted to audio\n",
        "    :param fftWindowSize: Size of FFT window used to create magnitudes\n",
        "    :param hopSize: Hop size in frames used to create magnitudes\n",
        "    :param phaseIterations: Number of Griffin-Lim iterations to recover phase\n",
        "    :param phase: If given, starts ISTFT with this particular phase matrix\n",
        "    :param length: If given, audio signal is clipped/padded to this number of frames\n",
        "    :return:\n",
        "    '''\n",
        "    if phase is not None:\n",
        "        if phaseIterations > 0:\n",
        "            # Refine audio given initial phase with a number of iterations\n",
        "            return reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations, phase, length)\n",
        "        # reconstructing the new complex matrix\n",
        "        stftMatrix = magnitude * np.exp(phase * 1j) # magnitude * e^(j*phase)\n",
        "        audio = librosa.istft(stftMatrix, hop_length=hopSize, length=length)\n",
        "    else:\n",
        "        audio = reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations)\n",
        "    return audio\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMVoj0i3024X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations=10, initPhase=None, length=None):\n",
        "    '''\n",
        "    Griffin-Lim algorithm for reconstructing the phase for a given magnitude spectrogram, optionally with a given\n",
        "    intial phase.\n",
        "    :param magnitude: Magnitudes to be converted to audio\n",
        "    :param fftWindowSize: Size of FFT window used to create magnitudes\n",
        "    :param hopSize: Hop size in frames used to create magnitudes\n",
        "    :param phaseIterations: Number of Griffin-Lim iterations to recover phase\n",
        "    :param initPhase: If given, starts reconstruction with this particular phase matrix\n",
        "    :param length: If given, audio signal is clipped/padded to this number of frames\n",
        "    :return:\n",
        "    '''\n",
        "    for i in range(phaseIterations):\n",
        "        if i == 0:\n",
        "            if initPhase is None:\n",
        "                reconstruction = np.random.random_sample(magnitude.shape) + 1j * (2 * np.pi * np.random.random_sample(magnitude.shape) - np.pi)\n",
        "            else:\n",
        "                reconstruction = np.exp(initPhase * 1j) # e^(j*phase), so that angle => phase\n",
        "        else:\n",
        "            reconstruction = librosa.stft(audio, fftWindowSize, hopSize)\n",
        "        spectrum = magnitude * np.exp(1j * np.angle(reconstruction))\n",
        "        if i == phaseIterations - 1:\n",
        "            audio = librosa.istft(spectrum, hopSize, length=length)\n",
        "        else:\n",
        "            audio = librosa.istft(spectrum, hopSize)\n",
        "    return audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgZh-AsTFspy",
        "colab_type": "text"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bS4zTic0_4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take_random_snippets(sample, keys, input_shape, num_samples):\n",
        "    # Take a sample (collection of audio files) and extract snippets from it at a number of random positions\n",
        "    start_pos = tf.random_uniform([num_samples], 0, maxval=sample[\"length\"] - input_shape[0], dtype=tf.int64)\n",
        "    return take_snippets_at_pos(sample, keys, start_pos, input_shape, num_samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCSurN5j1b17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take_all_snippets(sample, keys, input_shape, output_shape):\n",
        "    # Take a sample and extract snippets from the audio signals, using a hop size equal to the output size of the network\n",
        "    start_pos = tf.range(0, sample[\"length\"] - input_shape[0], delta=output_shape[0], dtype=tf.int64)\n",
        "    num_samples = start_pos.shape[0]\n",
        "    return take_snippets_at_pos(sample, keys, start_pos, input_shape, num_samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s-F-X4J1bmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take_snippets_at_pos(sample, keys, start_pos, input_shape, num_samples):\n",
        "    # Take a sample and extract snippets from the audio signals at the given start positions with the given number of samples width\n",
        "    batch = dict()\n",
        "    for key in keys:\n",
        "        batch[key] = tf.map_fn(lambda pos: sample[key][pos:pos + input_shape[0], :], start_pos, dtype=tf.float32)\n",
        "        batch[key].set_shape([num_samples, input_shape[0], input_shape[1]])\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NleE_8kA1ITQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _floats_feature(value):\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJQ3fwz01Jxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xdW9A5TeaM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_records(sample_list, model_config, input_shape, output_shape, records_path):\n",
        "    # Writes samples in the given list as TFrecords into a given path, using the current model config and in/output shapes\n",
        "\n",
        "    # Compute padding\n",
        "    if (input_shape[1] - output_shape[1]) % 2 != 0:\n",
        "        print(\"WARNING: Required number of padding of \" + str(input_shape[1] - output_shape[1]) + \" is uneven!\")\n",
        "    pad_frames = (input_shape[1] - output_shape[1]) // 2\n",
        "\n",
        "    # Set up writers\n",
        "    num_writers = 1\n",
        "    writers = [tf.python_io.TFRecordWriter(records_path + str(i) + \".tfrecords\") for i in range(num_writers)]\n",
        "\n",
        "    # Go through songs and write them to TFRecords\n",
        "    all_keys = model_config[\"source_names\"] + [\"mix\"]\n",
        "    for sample in sample_list:\n",
        "        print(\"Reading song\")\n",
        "        try:\n",
        "            audio_tracks = dict()\n",
        "\n",
        "            for key in all_keys:\n",
        "                audio, _ =load(sample[key], sr=model_config[\"expected_sr\"], mono=model_config[\"mono_downmix\"])\n",
        "\n",
        "                if not model_config[\"mono_downmix\"] and audio.shape[1] == 1:\n",
        "                    print(\"WARNING: Had to duplicate mono track to generate stereo\")\n",
        "                    audio = np.tile(audio, [1, 2])\n",
        "\n",
        "                audio_tracks[key] = audio\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(\"ERROR occurred during loading file \" + str(sample) + \". Skipping\")\n",
        "            continue\n",
        "\n",
        "        # Pad at beginning and end with zeros\n",
        "        audio_tracks = {key : np.pad(audio_tracks[key], [(pad_frames, pad_frames), (0, 0)], mode=\"constant\", constant_values=0.0) for key in list(audio_tracks.keys())}\n",
        "\n",
        "        # All audio tracks must be exactly same length and channels\n",
        "        length = audio_tracks[\"mix\"].shape[0]\n",
        "        channels = audio_tracks[\"mix\"].shape[1]\n",
        "        for audio in list(audio_tracks.values()):\n",
        "            assert(audio.shape[0] == length)\n",
        "            assert (audio.shape[1] == channels)\n",
        "\n",
        "        # Write to TFrecords the flattened version\n",
        "        feature = {key: _floats_feature(audio_tracks[key]) for key in all_keys}\n",
        "        feature[\"length\"] = _int64_feature(length)\n",
        "        feature[\"channels\"] = _int64_feature(channels)\n",
        "        sample = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "        writers[np.random.randint(0, num_writers)].write(sample.SerializeToString())\n",
        "\n",
        "    for writer in writers:\n",
        "        writer.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Ie0FoJV3S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_record(example_proto, source_names, shape):\n",
        "    # Parse record from TFRecord file\n",
        "\n",
        "    all_names = source_names + [\"mix\"]\n",
        "\n",
        "    features = {key : tf.FixedLenSequenceFeature([], allow_missing=True, dtype=tf.float32) for key in all_names}\n",
        "    features[\"length\"] = tf.FixedLenFeature([], tf.int64)\n",
        "    features[\"channels\"] = tf.FixedLenFeature([], tf.int64)\n",
        "\n",
        "    parsed_features = tf.parse_single_example(example_proto, features)\n",
        "\n",
        "    # Reshape\n",
        "    length = tf.cast(parsed_features[\"length\"], tf.int64)\n",
        "    channels = tf.constant(shape[-1], tf.int64) #tf.cast(parsed_features[\"channels\"], tf.int64)\n",
        "    sample = dict()\n",
        "    for key in all_names:\n",
        "        sample[key] = tf.reshape(parsed_features[key], tf.stack([length, channels]))\n",
        "    sample[\"length\"] = length\n",
        "    sample[\"channels\"] = channels\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCxYGbq5Ttyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getMUSDBHQ(directory):\n",
        "    subsets = list()\n",
        "    \n",
        "    output_path= os.path.join(directory, \"outputs\" ,\"getMUSDBHQ.csv\")\n",
        "    \n",
        "    if os.path.exists(output_path):\n",
        "          os.remove(output_path)\n",
        "    \n",
        "    with open(output_path, 'w', newline='') as file:\n",
        "            fieldnames = ['track', 'mix', 'drums', 'bass', 'others' ,'vocals', 'accompaniment']\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "    log_path= os.path.join(directory, \"outputs\" ,\"separation_info.csv\")\n",
        "    if not os.path.exists(log_path):\n",
        "      with open(log_path, 'w', newline='') as file:\n",
        "              fields = ['Track Name', 'Max abs Deviation', 'Mean abs Deviation']\n",
        "              writer = csv.writer(file)\n",
        "              writer.writerow(fields)\n",
        "        \n",
        "\n",
        "    for subset in [\"fake_train\", \"fake_test\"]:\n",
        "        print(\"\\n\\nLoading \" + subset + \" set...\")\n",
        "        subset_path = os.path.join(directory,subset)\n",
        "        samples = list()\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        for track in os.listdir(subset_path):\n",
        "            print(\"\\nProcessing track\\t\\t\\t\", track)\n",
        "            audio_path = os.path.join(subset_path,track)\n",
        "            audio, rate = stempeg.read_stems(audio_path)\n",
        "            \n",
        "            track_name= track[:-9]\n",
        "            target_path = os.path.join(directory,\"separated\",subset,track_name)\n",
        "            if not os.path.exists(target_path):\n",
        "               os.mkdir(target_path)\n",
        "\n",
        "            example = dict()\n",
        "            i=0;\n",
        "            for stem in [\"mix\", \"drums\", \"bass\", \"others\",\"vocals\"]:\n",
        "              stem_path = os.path.join(target_path,stem + \".wav\")\n",
        "              example[stem] = stem_path\n",
        "              if not os.path.exists(stem_path):                \n",
        "                sf.write(stem_path, audio[i], rate)\n",
        "              i=i+1\n",
        "            \n",
        "\n",
        "            acc_path = stem_path = os.path.join(target_path,  \"accompaniment.wav\")\n",
        "            if not os.path.exists(acc_path):\n",
        "              acc_audio = audio[1]+audio[2]+audio[3]\n",
        "              acc_audio = np.clip((acc_audio), -1.0, 1.0)\n",
        "              sf.write(acc_path, acc_audio, rate)\n",
        "              \n",
        "              diff_signal = np.abs(audio[0] - acc_audio - audio[4])\n",
        "              Max_abs_dev_from_source_additivity_constraint = np.max(diff_signal)\n",
        "              Mean_abs_dev_from_source_additivity_constraint = np.mean(diff_signal)\n",
        "              \n",
        "              with open(log_path, 'a') as csvfile:\n",
        "                filewriter = csv.writer(csvfile, delimiter=',')\n",
        "                filewriter.writerow([track_name, Max_abs_dev_from_source_additivity_constraint,Mean_abs_dev_from_source_additivity_constraint])\n",
        "            \n",
        "            example[\"accompaniment\"] = acc_path\n",
        "            output_csv = example\n",
        "            output_csv[\"track\"] = track_name\n",
        "            \n",
        "            with open(output_path, 'a', newline='') as file:\n",
        "              fieldnames = ['track', 'mix', 'drums', 'bass', 'others' ,'vocals', 'accompaniment']\n",
        "              writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "              writer.writerow(output_csv)\n",
        "\n",
        "            samples.append(example)\n",
        "\n",
        "        subsets.append(samples)\n",
        "\n",
        "    return subsets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zf8ctiH2CBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_path(db_path, instrument_node):\n",
        "    return db_path + os.path.sep + instrument_node.xpath(\"./relativeFilepath\")[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTtug-LWmUcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(model_config, input_shape, output_shape, partition):\n",
        "    \n",
        "    '''\n",
        "    For a model configuration and input/output shapes of the network, get the corresponding dataset for a given partition\n",
        "    :param model_config: Model config\n",
        "    :param input_shape: Input shape of network\n",
        "    :param output_shape: Output shape of network\n",
        "    :param partition: \"train\", \"valid\", or \"test\" partition\n",
        "    :return: Tensorflow dataset object\n",
        "    '''\n",
        "\n",
        "    # Check if pre-processed dataset is already available for this model config and partition\n",
        "    main_folder = model_config[\"directory\"]\n",
        "    if not os.path.exists(main_folder):\n",
        "        os.makedirs(main_folder)\n",
        "        # We have to prepare the MUSDB dataset\n",
        "        print(\"Preparing MUSDB dataset! This could take a while...\")\n",
        "        dsd_train, dsd_test = getMUSDBHQ(model_config[\"musdb_path\"])  # List of (mix, acc, bass, drums, other, vocal) tuples\n",
        "\n",
        "        # Pick 25 random songs for validation from MUSDB train set (this is always the same selection each time since we fix the random seed!)\n",
        "        val_idx = np.random.choice(len(dsd_train), size=1, replace=False)\n",
        "        train_idx = [i for i in range(len(dsd_train)) if i not in val_idx]\n",
        "        print(\"Validation with MUSDB training songs no. \" + str(val_idx))\n",
        "\n",
        "        # Draw randomly from datasets\n",
        "        dataset = dict()\n",
        "        dataset[\"train\"] = [dsd_train[i] for i in train_idx]\n",
        "        dataset[\"valid\"] = [dsd_train[i] for i in val_idx]\n",
        "        dataset[\"test\"] = dsd_test    \n",
        "\n",
        "\n",
        "        # Convert audio files into TFRecords now\n",
        "\n",
        "        # The dataset structure is a dictionary with \"train\", \"valid\", \"test\" keys, whose entries are lists, where each element represents a song.\n",
        "        # Each song is represented as a dictionary containing elements mix, acc, vocal or mix, bass, drums, other, vocal depending on the task.\n",
        "\n",
        "        num_cores = 8\n",
        "\n",
        "        for curr_partition in [\"train\", \"valid\", \"test\"]:\n",
        "            print(\"Writing \" + curr_partition + \" partition...\")\n",
        "\n",
        "            # Shuffle sample order\n",
        "            sample_list = dataset[curr_partition]\n",
        "            random.shuffle(sample_list)\n",
        "\n",
        "            # Create folder\n",
        "            partition_folder = os.path.join(main_folder, curr_partition)\n",
        "            os.makedirs(partition_folder)\n",
        "\n",
        "            part_entries = int(np.ceil(float(len(sample_list) / float(num_cores))))\n",
        "            processes = list()\n",
        "            for core in range(num_cores):\n",
        "                train_filename = os.path.join(partition_folder, str(core) + \"_\")  # address to save the TFRecords file\n",
        "                sample_list_subset = sample_list[core * part_entries:min((core + 1) * part_entries, len(sample_list))]\n",
        "                proc = Process(target=write_records,\n",
        "                               args=(sample_list_subset, model_config, input_shape, output_shape, train_filename))\n",
        "                proc.start()\n",
        "                processes.append(proc)\n",
        "            for p in processes:\n",
        "                p.join()\n",
        "\n",
        "    print(\"Dataset ready!\")\n",
        "    # Finally, load TFRecords dataset based on the desired partition\n",
        "    dataset_folder = os.path.join(main_folder, partition)\n",
        "   \n",
        "    \n",
        "    records_files = glob.glob(os.path.join(dataset_folder, \"*.tfrecords\"))\n",
        "    random.shuffle(records_files)\n",
        "    dataset = tf.data.TFRecordDataset(records_files)\n",
        "    dataset = dataset.map(lambda x : parse_record(x, model_config[\"source_names\"], input_shape[1:]), num_parallel_calls=model_config[\"num_workers\"])\n",
        "    dataset = dataset.prefetch(10)\n",
        "\n",
        "    # Take random samples from each song\n",
        "    if partition == \"train\":\n",
        "        dataset = dataset.flat_map(lambda x : take_random_snippets(x, model_config[\"source_names\"] + [\"mix\"], input_shape[1:], model_config[\"num_snippets_per_track\"]))\n",
        "    else:\n",
        "        dataset = dataset.flat_map(lambda x : take_all_snippets(x, model_config[\"source_names\"] + [\"mix\"], input_shape[1:], output_shape[1:]))\n",
        "    dataset = dataset.prefetch(100)\n",
        "\n",
        "    if partition == \"train\" and model_config[\"augmentation\"]: # If its the train partition, activate data augmentation if desired\n",
        "            dataset = dataset.map(random_amplify, num_parallel_calls=model_config[\"num_workers\"]).prefetch(100)\n",
        "\n",
        "    # Cut source outputs to centre part\n",
        "    dataset = dataset.map(lambda x : crop_sample(x, (input_shape[1] - output_shape[1])//2)).prefetch(100)\n",
        "\n",
        "    if partition == \"train\": # Repeat endlessly and shuffle when training\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.shuffle(buffer_size=model_config[\"cache_size\"])\n",
        "\n",
        "    dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(model_config[\"batch_size\"]))\n",
        "    dataset = dataset.prefetch(1)\n",
        "\n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXaKryCsGEbq",
        "colab_type": "text"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC7zQBwki0DG",
        "colab_type": "text"
      },
      "source": [
        "### **Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkh_g9VHi2t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def independent_outputs(featuremap, source_names, num_channels, filter_width, padding, activation):\n",
        "    outputs = dict()\n",
        "    for name in source_names:\n",
        "        outputs[name] = tf.layers.conv1d(featuremap, num_channels, filter_width, activation=activation, padding=padding)\n",
        "    return outputs\n",
        "\n",
        "def difference_output(input_mix, featuremap, source_names, num_channels, filter_width, padding, activation, training):\n",
        "    outputs = dict()\n",
        "    sum_source = 0\n",
        "    for name in source_names[:-1]:\n",
        "        out = tf.layers.conv1d(featuremap, num_channels, filter_width, activation=activation, padding=padding)\n",
        "        outputs[name] = out\n",
        "        sum_source = sum_source + out\n",
        "\n",
        "    # Compute last source based on the others\n",
        "    last_source = crop(input_mix, sum_source.get_shape().as_list()) - sum_source\n",
        "    last_source = AudioClip(last_source, training)\n",
        "    outputs[source_names[-1]] = last_source\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HW0UyCzil8u",
        "colab_type": "text"
      },
      "source": [
        "### **Interpolation Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnVqXbzSiqIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learned_interpolation_layer(input, padding, level):\n",
        "    '''\n",
        "    Implements a trainable upsampling layer by interpolation by a factor of two, from N samples to N*2 - 1.\n",
        "    Interpolation of intermediate feature vectors v_1 and v_2 (of dimensionality F) is performed by\n",
        "     w \\cdot v_1 + (1-w) \\cdot v_2, where \\cdot is point-wise multiplication, and w an F-dimensional weight vector constrained to [0,1]\n",
        "    :param input: Input features of shape [batch_size, 1, width, F]\n",
        "    :param padding:\n",
        "    :param level:\n",
        "    :return:\n",
        "    '''\n",
        "    assert(padding == \"valid\" or padding == \"same\")\n",
        "    features = input.get_shape().as_list()[3]\n",
        "\n",
        "    # Construct 2FxF weight matrix, where F is the number of feature channels in the feature map.\n",
        "    # Matrix is constrained, made up out of two diagonal FxF matrices with diagonal weights w and 1-w. w is constrained to be in [0,1] # mioid\n",
        "    weights = tf.get_variable(\"interp_\" + str(level), shape=[features], dtype=tf.float32)\n",
        "    weights_scaled = tf.nn.sigmoid(weights) # Constrain weights to [0,1]\n",
        "    counter_weights = 1.0 - weights_scaled # Mirrored weights for the features from the other time step\n",
        "    conv_weights = tf.expand_dims(tf.concat([tf.expand_dims(tf.diag(weights_scaled), axis=0), tf.expand_dims(tf.diag(counter_weights), axis=0)], axis=0), axis=0)\n",
        "    intermediate_vals = tf.nn.conv2d(input, conv_weights, strides=[1,1,1,1], padding=padding.upper())\n",
        "\n",
        "    intermediate_vals = tf.transpose(intermediate_vals, [2, 0, 1, 3])\n",
        "    out = tf.transpose(input, [2, 0, 1, 3])\n",
        "    num_entries = out.get_shape().as_list()[0]\n",
        "    out = tf.concat([out, intermediate_vals], axis=0)\n",
        "    indices = list()\n",
        "\n",
        "    # Interleave interpolated features with original ones, starting with the first original one\n",
        "    num_outputs = (2*num_entries - 1) if padding == \"valid\" else 2*num_entries\n",
        "    for idx in range(num_outputs):\n",
        "        if idx % 2 == 0:\n",
        "            indices.append(idx // 2)\n",
        "        else:\n",
        "            indices.append(num_entries + idx//2)\n",
        "    out = tf.gather(out, indices)\n",
        "    current_layer = tf.transpose(out, [1, 2, 0, 3])\n",
        "    return current_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAc4svG-Gw4P",
        "colab_type": "text"
      },
      "source": [
        "**Unet Audio Separator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKZvGbnRGHF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetAudioSeparator:\n",
        "    '''\n",
        "    U-Net separator network for singing voice separation.\n",
        "    Uses valid convolutions, so it predicts for the centre part of the input - only certain input and output shapes are therefore possible (see getpadding function)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, model_config):\n",
        "        '''\n",
        "        Initialize U-net\n",
        "        :param num_layers: Number of down- and upscaling layers in the network \n",
        "        '''\n",
        "        self.num_layers = model_config[\"num_layers\"]\n",
        "        self.num_initial_filters = model_config[\"num_initial_filters\"]\n",
        "        self.filter_size = model_config[\"filter_size\"]\n",
        "        self.merge_filter_size = model_config[\"merge_filter_size\"]\n",
        "        self.input_filter_size = model_config[\"input_filter_size\"]\n",
        "        self.output_filter_size = model_config[\"output_filter_size\"]\n",
        "        self.upsampling = model_config[\"upsampling\"]\n",
        "        self.output_type = model_config[\"output_type\"]\n",
        "        self.context = model_config[\"context\"]\n",
        "        self.padding = \"valid\" if model_config[\"context\"] else \"same\"\n",
        "        self.source_names = model_config[\"source_names\"]\n",
        "        self.num_channels = 1 if model_config[\"mono_downmix\"] else 2\n",
        "        self.output_activation = model_config[\"output_activation\"]\n",
        "\n",
        "    def get_padding(self, shape):\n",
        "        '''\n",
        "        Calculates the required amounts of padding along each axis of the input and output, so that the Unet works and has the given shape as output shape\n",
        "        :param shape: Desired output shape \n",
        "        :return: Input_shape, output_shape, where each is a list [batch_size, time_steps, channels]\n",
        "        '''\n",
        "\n",
        "        if self.context:\n",
        "            # Check if desired shape is possible as output shape - go from output shape towards lowest-res feature map\n",
        "            rem = float(shape[1]) # Cut off batch size number and channel\n",
        "\n",
        "            # Output filter size\n",
        "            rem = rem - self.output_filter_size + 1\n",
        "\n",
        "            # Upsampling blocks\n",
        "            for i in range(self.num_layers):\n",
        "                rem = rem + self.merge_filter_size - 1\n",
        "                rem = (rem + 1.) / 2.# out = in + in - 1 <=> in = (out+1)/\n",
        "\n",
        "            # Round resulting feature map dimensions up to nearest integer\n",
        "            x = np.asarray(np.ceil(rem),dtype=np.int64)\n",
        "            assert(x >= 2)\n",
        "\n",
        "            # Compute input and output shapes based on lowest-res feature map\n",
        "            output_shape = x\n",
        "            input_shape = x\n",
        "\n",
        "            # Extra conv\n",
        "            input_shape = input_shape + self.filter_size - 1\n",
        "\n",
        "            # Go from centre feature map through up- and downsampling blocks\n",
        "            for i in range(self.num_layers):\n",
        "                output_shape = 2*output_shape - 1 #Upsampling\n",
        "                output_shape = output_shape - self.merge_filter_size + 1 # Conv\n",
        "\n",
        "                input_shape = 2*input_shape - 1 # Decimation\n",
        "                if i < self.num_layers - 1:\n",
        "                    input_shape = input_shape + self.filter_size - 1 # Conv\n",
        "                else:\n",
        "                    input_shape = input_shape + self.input_filter_size - 1\n",
        "\n",
        "            # Output filters\n",
        "            output_shape = output_shape - self.output_filter_size + 1\n",
        "\n",
        "            input_shape = np.concatenate([[shape[0]], [input_shape], [self.num_channels]])\n",
        "            output_shape = np.concatenate([[shape[0]], [output_shape], [self.num_channels]])\n",
        "\n",
        "            return input_shape, output_shape\n",
        "        else:\n",
        "            return [shape[0], shape[1], self.num_channels], [shape[0], shape[1], self.num_channels]\n",
        "\n",
        "    def get_output(self, input, training, return_spectrogram=False, reuse=True):\n",
        "        '''\n",
        "        Creates symbolic computation graph of the U-Net for a given input batch\n",
        "        :param input: Input batch of mixtures, 3D tensor [batch_size, num_samples, num_channels]\n",
        "        :param reuse: Whether to create new parameter variables or reuse existing ones\n",
        "        :return: U-Net output: List of source estimates. Each item is a 3D tensor [batch_size, num_out_samples, num_channels]\n",
        "        '''\n",
        "        with tf.variable_scope(\"separator\", reuse=reuse):\n",
        "            enc_outputs = list()\n",
        "            current_layer = input\n",
        "\n",
        "            # Down-convolution: Repeat strided conv\n",
        "            for i in range(self.num_layers):\n",
        "                current_layer = tf.layers.conv1d(current_layer, self.num_initial_filters + (self.num_initial_filters * i), self.filter_size, strides=1, activation=LeakyReLU, padding=self.padding) # out = in - filter + 1\n",
        "                enc_outputs.append(current_layer)\n",
        "                current_layer = current_layer[:,::2,:] # Decimate by factor of 2 # out = (in-1)/2 + 1\n",
        "\n",
        "            current_layer = tf.layers.conv1d(current_layer, self.num_initial_filters + (self.num_initial_filters * self.num_layers),self.filter_size,activation=LeakyReLU,padding=self.padding) # One more conv here since we need to compute features after last decimation\n",
        "\n",
        "            # Feature map here shall be X along one dimension\n",
        "\n",
        "            # Upconvolution\n",
        "            for i in range(self.num_layers):\n",
        "                #UPSAMPLING\n",
        "                current_layer = tf.expand_dims(current_layer, axis=1)\n",
        "                if self.upsampling == 'learned':\n",
        "                    # Learned interpolation between two neighbouring time positions by using a convolution filter of width 2, and inserting the responses in the middle of the two respective inputs\n",
        "                    current_layer = learned_interpolation_layer(current_layer, self.padding, i)\n",
        "                else:\n",
        "                    if self.context:\n",
        "                        current_layer = tf.image.resize_bilinear(current_layer, [1, current_layer.get_shape().as_list()[2] * 2 - 1], align_corners=True)\n",
        "                    else:\n",
        "                        current_layer = tf.image.resize_bilinear(current_layer, [1, current_layer.get_shape().as_list()[2]*2]) # out = in + in - 1\n",
        "                current_layer = tf.squeeze(current_layer, axis=1)\n",
        "                # UPSAMPLING FINISHED\n",
        "\n",
        "                assert(enc_outputs[-i-1].get_shape().as_list()[1] == current_layer.get_shape().as_list()[1] or self.context) #No cropping should be necessary unless we are using context\n",
        "                current_layer = crop_and_concat(enc_outputs[-i-1], current_layer, match_feature_dim=False)\n",
        "                current_layer = tf.layers.conv1d(current_layer, self.num_initial_filters + (self.num_initial_filters * (self.num_layers - i - 1)), self.merge_filter_size,\n",
        "                                                 activation=LeakyReLU,\n",
        "                                                 padding=self.padding)  # out = in - filter + 1\n",
        "\n",
        "            current_layer = crop_and_concat(input, current_layer, match_feature_dim=False)\n",
        "\n",
        "            # Output layer\n",
        "            # Determine output activation function\n",
        "            if self.output_activation == \"tanh\":\n",
        "                out_activation = tf.tanh\n",
        "            elif self.output_activation == \"linear\":\n",
        "                out_activation = lambda x: AudioClip(x, training)\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            if self.output_type == \"direct\":\n",
        "                return independent_outputs(current_layer, self.source_names, self.num_channels, self.output_filter_size, self.padding, out_activation)\n",
        "            elif self.output_type == \"difference\":\n",
        "                cropped_input = crop(input,current_layer.get_shape().as_list(), match_feature_dim=False)\n",
        "                return difference_output(cropped_input, current_layer, self.source_names, self.num_channels, self.output_filter_size, self.padding, out_activation, training)\n",
        "            else:\n",
        "                raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGkJbzACFwl1",
        "colab_type": "text"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWrvAX6oF66s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def config(task_name):\n",
        "\n",
        "  model_config[\"task\"]=task_name\n",
        "  if model_config[\"task\"] == \"multi_instrument\":\n",
        "      model_config[\"source_names\"] = [\"bass\", \"drums\", \"other\", \"vocals\"]\n",
        "  elif model_config[\"task\"] == \"voice\":\n",
        "      model_config[\"source_names\"] = [\"accompaniment\", \"vocals\"]\n",
        "  else:\n",
        "      raise NotImplementedError\n",
        "  model_config[\"num_sources\"] = len(model_config[\"source_names\"])\n",
        "  model_config[\"num_channels\"] = 1 if model_config[\"mono_downmix\"] else 2\n",
        "\n",
        "\n",
        "def train(model_config, load_model=None):\n",
        "    # Determine input and output shapes\n",
        "    disc_input_shape = [model_config[\"batch_size\"], model_config[\"num_frames\"], 0]  # Shape of input\n",
        "    separator_class = UnetAudioSeparator(model_config)\n",
        "\n",
        "\n",
        "    sep_input_shape, sep_output_shape = separator_class.get_padding(np.array(disc_input_shape))\n",
        "    separator_func = separator_class.get_output\n",
        "\n",
        "    dataset = get_dataset(model_config, sep_input_shape, sep_output_shape, partition=\"train\")\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    batch = iterator.get_next()\n",
        "\n",
        "    print(\"Training...\")\n",
        "\n",
        "    separator_sources = separator_func(batch[\"mix\"], True, not model_config[\"raw_audio_loss\"], reuse=tf.AUTO_REUSE)\n",
        "    separator_loss = 0\n",
        "\n",
        "    for key in model_config[\"source_names\"]:\n",
        "        real_source = batch[key]\n",
        "        sep_source = separator_sources[key]\n",
        "        separator_loss += tf.reduce_mean(tf.square(real_source - sep_source))\n",
        "    separator_loss = separator_loss / float(model_config[\"num_sources\"]) # Normalise by number of sources\n",
        "\n",
        "    # TRAINING CONTROL VARIABLES\n",
        "    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False, dtype=tf.int64)\n",
        "    increment_global_step = tf.assign(global_step, global_step + 1)\n",
        "\n",
        "    # Set up optimizers\n",
        "    separator_vars = getTrainableVariables(\"separator\")\n",
        "    print(\"Sep_Vars: \" + str(getNumParams(separator_vars)))\n",
        "    print(\"Num of variables \" + str(len(tf.global_variables())))\n",
        "\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        with tf.variable_scope(\"separator_solver\"):\n",
        "            separator_solver = tf.train.AdamOptimizer(learning_rate=model_config[\"init_sup_sep_lr\"]).minimize(separator_loss, var_list=separator_vars)\n",
        "    \n",
        "    # SUMMARIES\n",
        "    tf.summary.scalar(\"sep_loss\", separator_loss, collections=[\"sup\"])\n",
        "    sup_summaries = tf.summary.merge_all(key='sup')\n",
        "\n",
        "    # Start session and queue input threads\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth=True\n",
        "    sess = tf.Session(config=config)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    writer = tf.summary.FileWriter(model_config[\"log_dir\"] + os.path.sep + str(experiment_id),graph=sess.graph)\n",
        "\n",
        "    # CHECKPOINTING\n",
        "    # Load pretrained model to continue training, if we are supposed to\n",
        "    if load_model != None:\n",
        "        restorer = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
        "        print(\"Num of variables\" + str(len(tf.global_variables())))\n",
        "        restorer.restore(sess, load_model)\n",
        "        print('Pre-trained model restored from file ' + load_model)\n",
        "\n",
        "    saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
        "\n",
        "    # Start training loop\n",
        "    _global_step = sess.run(global_step)\n",
        "    _init_step = _global_step\n",
        "    \n",
        "    # Epoch finished - Save model\n",
        "    print(\"Finished epoch!\")\n",
        "    save_path = saver.save(sess, model_config[\"model_base_dir\"] + os.path.sep + str(experiment_id) + os.path.sep + str(experiment_id), global_step=int(_global_step))\n",
        "\n",
        "    # Close session, clear computational graph\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "    sess.close()\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    return save_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTqSXxpoHr_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2a337339-e617-4aec-9209-895971dd1cd3"
      },
      "source": [
        "config(\"voice\")\n",
        "train(model_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset ready!\n",
            "Training...\n",
            "Sep_Vars: 10263028\n",
            "Num of variables 55\n",
            "Finished epoch!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/BTP/Dataset/checkpoints/876218/876218-0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}